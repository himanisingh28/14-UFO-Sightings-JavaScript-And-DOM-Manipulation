{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDxHE3c9VRsT"
      },
      "source": [
        "# Module 2: Deep Learning Primer\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this module, you will be able to:\n",
        "- Understand the fundamental concepts of neural networks\n",
        "- Recognize common neural network architectures\n",
        "- Understand how networks learn through gradient descent and backpropagation\n",
        "- Build a simple image classifier with TensorFlow/Keras\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltu2LNUXVRsT"
      },
      "source": [
        "## 1. From ML to Deep Learning\n",
        "\n",
        "### What Makes Deep Learning \"Deep\"?\n",
        "\n",
        "**Traditional Machine Learning:**\n",
        "- Requires manual **feature engineering** (humans decide what patterns to look for)\n",
        "- Works well with structured, tabular data\n",
        "- Limited ability to handle raw data like images, audio, text\n",
        "\n",
        "**Deep Learning:**\n",
        "- **Automatic feature learning** from raw data\n",
        "- Uses multiple layers (hence \"deep\") to learn hierarchical representations\n",
        "- Excels at unstructured data: images, speech, text, video\n",
        "\n",
        "```\n",
        "Image ‚Üí [Low-level: edges] ‚Üí [Mid-level: shapes] ‚Üí [High-level: objects] ‚Üí \"Cat\"\n",
        "Text  ‚Üí [Characters] ‚Üí [Words] ‚Üí [Phrases] ‚Üí [Meaning/Context]\n",
        "```\n",
        "\n",
        "### When to Use Deep Learning?\n",
        "\n",
        "| Use Traditional ML | Use Deep Learning |\n",
        "|-------------------|-------------------|\n",
        "| Small datasets (<10K samples) | Large datasets (100K+) |\n",
        "| Structured/tabular data | Unstructured data (images, text, audio) |\n",
        "| Need interpretability | Performance is priority |\n",
        "| Limited compute resources | Have GPUs available |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx_XaffTVRsU"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. The Artificial Neuron\n",
        "\n",
        "### Inspired by Biology\n",
        "\n",
        "An artificial neuron mimics (loosely) how biological neurons work:\n",
        "\n",
        "```\n",
        "Biological:  Dendrites ‚Üí Cell Body ‚Üí Axon ‚Üí Synapses\n",
        "Artificial:  Inputs ‚Üí Weighted Sum ‚Üí Activation ‚Üí Output\n",
        "```\n",
        "\n",
        "### Mathematical Model\n",
        "\n",
        "```\n",
        "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "     x‚ÇÅ ‚îÄ‚îÄw‚ÇÅ‚îÄ‚îÄ‚ñ∂    ‚îÇ                     ‚îÇ\n",
        "     x‚ÇÇ ‚îÄ‚îÄw‚ÇÇ‚îÄ‚îÄ‚ñ∂    ‚îÇ  z = Œ£(w·µ¢¬∑x·µ¢) + b  ‚îÇ ‚îÄ‚îÄ‚ñ∂ a = œÉ(z) ‚îÄ‚îÄ‚ñ∂ output\n",
        "     x‚ÇÉ ‚îÄ‚îÄw‚ÇÉ‚îÄ‚îÄ‚ñ∂    ‚îÇ                     ‚îÇ\n",
        "          ‚¨Ü        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "        bias (b)\n",
        "```\n",
        "\n",
        "Where:\n",
        "- **x·µ¢**: Input features\n",
        "- **w·µ¢**: Weights (learnable parameters)\n",
        "- **b**: Bias (another learnable parameter)\n",
        "- **œÉ**: Activation function (introduces non-linearity)\n",
        "\n",
        "### Activation Functions\n",
        "\n",
        "Without activation functions, a neural network would just be linear transformations stacked together (equivalent to a single linear function). Activations introduce **non-linearity**:\n",
        "\n",
        "- **ReLU** (Rectified Linear Unit): `max(0, x)` - Most common\n",
        "- **Sigmoid**: `1 / (1 + e^(-x))` - Output between 0 and 1\n",
        "- **Tanh**: `(e^x - e^(-x)) / (e^x + e^(-x))` - Output between -1 and 1\n",
        "- **Softmax**: Converts outputs to probability distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVq3qCcQVRsU"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q tensorflow matplotlib numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewq7o_3VVRsU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize activation functions\n",
        "x = np.linspace(-5, 5, 100)\n",
        "\n",
        "# Define activation functions\n",
        "relu = np.maximum(0, x)\n",
        "sigmoid = 1 / (1 + np.exp(-x))\n",
        "tanh = np.tanh(x)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "axes[0].plot(x, relu, 'b-', linewidth=2)\n",
        "axes[0].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
        "axes[0].axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
        "axes[0].set_title('ReLU: max(0, x)', fontsize=12)\n",
        "axes[0].set_xlabel('z')\n",
        "axes[0].set_ylabel('a = ReLU(z)')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(x, sigmoid, 'g-', linewidth=2)\n",
        "axes[1].axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n",
        "axes[1].axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
        "axes[1].set_title('Sigmoid: 1/(1+e^(-x))', fontsize=12)\n",
        "axes[1].set_xlabel('z')\n",
        "axes[1].set_ylabel('a = œÉ(z)')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].plot(x, tanh, 'm-', linewidth=2)\n",
        "axes[2].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
        "axes[2].axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
        "axes[2].set_title('Tanh', fontsize=12)\n",
        "axes[2].set_xlabel('z')\n",
        "axes[2].set_ylabel('a = tanh(z)')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üí° ReLU is the most popular because it's simple, fast, and helps avoid vanishing gradients!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBL2EyxBVRsU"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Neural Network Architecture\n",
        "\n",
        "A neural network is layers of neurons connected together:\n",
        "\n",
        "```\n",
        "Input Layer          Hidden Layers              Output Layer\n",
        "    ‚óã                    ‚óã    ‚óã                     ‚óã\n",
        "    ‚óã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂    ‚óã    ‚óã  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂    ‚óã\n",
        "    ‚óã                    ‚óã    ‚óã                     ‚óã\n",
        "    ‚óã                    ‚óã    ‚óã                     \n",
        "(features)           (learned                   (predictions)\n",
        "                    representations)\n",
        "```\n",
        "\n",
        "### Common Architectures\n",
        "\n",
        "| Architecture | Best For | Key Idea |\n",
        "|-------------|----------|----------|\n",
        "| **Dense/MLP** | Tabular data, simple tasks | Fully connected layers |\n",
        "| **CNN** | Images, spatial data | Convolutions detect local patterns |\n",
        "| **RNN/LSTM** | Sequences, time series | Memory of previous inputs |\n",
        "| **Transformer** | Text, long sequences | Attention mechanism (GPT, BERT) |\n",
        "\n",
        "> **üîë For Generative AI:** Transformers are the foundation of modern LLMs (GPT, BERT, LLaMA, Claude)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0heLfjjuVRsV"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. How Neural Networks Learn\n",
        "\n",
        "### The Learning Loop\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  1. FORWARD PASS                                            ‚îÇ\n",
        "‚îÇ     Input ‚Üí Network ‚Üí Prediction                            ‚îÇ\n",
        "‚îÇ                                                             ‚îÇ\n",
        "‚îÇ  2. LOSS CALCULATION                                        ‚îÇ\n",
        "‚îÇ     How wrong was the prediction? (Loss = f(prediction, y)) ‚îÇ\n",
        "‚îÇ                                                             ‚îÇ\n",
        "‚îÇ  3. BACKWARD PASS (Backpropagation)                         ‚îÇ\n",
        "‚îÇ     Calculate gradients: ‚àÇLoss/‚àÇweights                     ‚îÇ\n",
        "‚îÇ                                                             ‚îÇ\n",
        "‚îÇ  4. UPDATE WEIGHTS (Gradient Descent)                       ‚îÇ\n",
        "‚îÇ     weights = weights - learning_rate √ó gradient            ‚îÇ\n",
        "‚îÇ                                                             ‚îÇ\n",
        "‚îÇ  Repeat for many iterations (epochs)                        ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "**Loss Function:** Measures how wrong predictions are\n",
        "- Classification: Cross-entropy loss\n",
        "- Regression: Mean squared error\n",
        "\n",
        "**Gradient Descent:** Finds the direction to adjust weights to reduce loss\n",
        "- Think of it as rolling downhill on the loss landscape\n",
        "\n",
        "**Learning Rate:** How big of steps to take\n",
        "- Too high: Overshoots optimal values\n",
        "- Too low: Training takes forever\n",
        "\n",
        "**Backpropagation:** Efficiently computes gradients using chain rule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmrJj8IUVRsV"
      },
      "outputs": [],
      "source": [
        "# Visualizing Gradient Descent on a simple loss landscape\n",
        "def loss_function(w):\n",
        "    \"\"\"Simple quadratic loss function\"\"\"\n",
        "    return (w - 3) ** 2 + 2\n",
        "\n",
        "def gradient(w):\n",
        "    \"\"\"Derivative of loss function\"\"\"\n",
        "    return 2 * (w - 3)\n",
        "\n",
        "# Gradient descent simulation\n",
        "w = -2  # Starting point\n",
        "learning_rate = 0.1\n",
        "history = [w]\n",
        "\n",
        "for _ in range(20):\n",
        "    grad = gradient(w)\n",
        "    w = w - learning_rate * grad  # Update rule\n",
        "    history.append(w)\n",
        "\n",
        "# Visualize\n",
        "w_range = np.linspace(-3, 8, 100)\n",
        "loss_values = loss_function(w_range)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(w_range, loss_values, 'b-', linewidth=2, label='Loss landscape')\n",
        "plt.plot(history, [loss_function(w) for w in history], 'ro-', markersize=8, label='Gradient descent steps')\n",
        "plt.axvline(x=3, color='g', linestyle='--', alpha=0.7, label='Optimal w=3')\n",
        "plt.xlabel('Weight (w)', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.title('Gradient Descent: Finding the Minimum', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Started at w={history[0]:.2f}, converged to w={history[-1]:.2f}\")\n",
        "print(f\"Optimal value is w=3.00\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCU55TjMVRsV"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Optional Demo: TensorFlow Playground\n",
        "\n",
        "üéÆ **Interactive Exploration:**\n",
        "\n",
        "Visit [TensorFlow Playground](https://playground.tensorflow.org/) to:\n",
        "\n",
        "1. See how adding layers and neurons affects learning\n",
        "2. Experiment with different activation functions\n",
        "3. Watch the decision boundary evolve during training\n",
        "4. Understand feature learning visually\n",
        "\n",
        "**Try These Experiments:**\n",
        "1. Start with the spiral dataset and 1 hidden layer - can you fit it?\n",
        "2. Add more layers - how does it change?\n",
        "3. Try ReLU vs Sigmoid - which learns faster?\n",
        "4. What happens with too high learning rate?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yarw1spFVRsV"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Hands-On: Image Classification with TensorFlow\n",
        "\n",
        "Let's build a neural network to classify handwritten digits (MNIST dataset).\n",
        "\n",
        "MNIST contains 70,000 grayscale images of digits 0-9, each 28√ó28 pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REO4gySUVRsV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Load MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "print(f\"\\nüìä Dataset loaded:\")\n",
        "print(f\"   Training samples: {X_train.shape[0]}\")\n",
        "print(f\"   Test samples: {X_test.shape[0]}\")\n",
        "print(f\"   Image shape: {X_train.shape[1:]} (28√ó28 pixels, grayscale)\")\n",
        "\n",
        "# Visualize some samples\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(X_train[i], cmap='gray')\n",
        "    ax.set_title(f'Label: {y_train[i]}', fontsize=11)\n",
        "    ax.axis('off')\n",
        "plt.suptitle('Sample MNIST Images', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adctmQ2wVRsW"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data\n",
        "\n",
        "# 1. Normalize pixel values to 0-1 range (originally 0-255)\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# 2. Flatten images from 28√ó28 to 784-dimensional vectors\n",
        "#    (for our simple dense network)\n",
        "X_train_flat = X_train.reshape(-1, 28 * 28)\n",
        "X_test_flat = X_test.reshape(-1, 28 * 28)\n",
        "\n",
        "print(f\"Preprocessed shapes:\")\n",
        "print(f\"  X_train: {X_train_flat.shape}\")\n",
        "print(f\"  X_test: {X_test_flat.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPoE2LHjVRsW"
      },
      "outputs": [],
      "source": [
        "# Build the Neural Network\n",
        "\n",
        "model = keras.Sequential([\n",
        "    # Input layer (784 features = 28√ó28 pixels)\n",
        "    layers.Input(shape=(784,)),\n",
        "\n",
        "    # Hidden layer 1: 128 neurons with ReLU activation\n",
        "    layers.Dense(128, activation='relu', name='hidden_1'),\n",
        "\n",
        "    # Hidden layer 2: 64 neurons with ReLU activation\n",
        "    layers.Dense(64, activation='relu', name='hidden_2'),\n",
        "\n",
        "    # Output layer: 10 neurons (one per digit) with softmax for probabilities\n",
        "    layers.Dense(10, activation='softmax', name='output')\n",
        "])\n",
        "\n",
        "# Display model architecture\n",
        "model.summary()\n",
        "\n",
        "print(\"\\nüí° Total parameters to learn:\", model.count_params())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f-pW0TIVRsW"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',  # Popular adaptive optimizer\n",
        "    loss='sparse_categorical_crossentropy',  # For multi-class classification\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model compiled!\")\n",
        "print(\"   Optimizer: Adam (adaptive learning rate)\")\n",
        "print(\"   Loss: Sparse Categorical Crossentropy\")\n",
        "print(\"   Metric: Accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpaPKSGhVRsW"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(\"üéì Training the neural network...\\n\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_flat, y_train,\n",
        "    epochs=10,  # Number of full passes through training data\n",
        "    batch_size=32,  # How many samples per gradient update\n",
        "    validation_split=0.1,  # Use 10% of training data for validation\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8iqJnL_VRsW"
      },
      "outputs": [],
      "source": [
        "# Visualize training progress\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Accuracy plot\n",
        "axes[0].plot(history.history['accuracy'], 'b-', label='Training')\n",
        "axes[0].plot(history.history['val_accuracy'], 'r-', label='Validation')\n",
        "axes[0].set_title('Model Accuracy Over Epochs', fontsize=12)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss plot\n",
        "axes[1].plot(history.history['loss'], 'b-', label='Training')\n",
        "axes[1].plot(history.history['val_loss'], 'r-', label='Validation')\n",
        "axes[1].set_title('Model Loss Over Epochs', fontsize=12)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5dfmwW9VRsW"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test_flat, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nFinal Test Results:\")\n",
        "print(f\"   Test Accuracy: {test_accuracy:.2%}\")\n",
        "print(f\"   Test Loss: {test_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIggwO0zVRsW"
      },
      "outputs": [],
      "source": [
        "# Make predictions and visualize\n",
        "predictions = model.predict(X_test_flat[:10], verbose=0)\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(X_test[i], cmap='gray')\n",
        "    pred_digit = np.argmax(predictions[i])\n",
        "    confidence = predictions[i][pred_digit]\n",
        "    true_digit = y_test[i]\n",
        "\n",
        "    color = 'green' if pred_digit == true_digit else 'red'\n",
        "    ax.set_title(f'Pred: {pred_digit} ({confidence:.0%})\\nTrue: {true_digit}',\n",
        "                 fontsize=10, color=color)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Model Predictions on Test Images', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClobnPJ3VRsW"
      },
      "source": [
        "---\n",
        "\n",
        "## 7. Why Deep Learning Powers Generative AI\n",
        "\n",
        "The concepts we learned today are the foundation of LLMs and generative models:\n",
        "\n",
        "| Deep Learning Concept | Generative AI Application |\n",
        "|----------------------|---------------------------|\n",
        "| Neural networks | GPT, BERT, LLaMA are massive neural networks |\n",
        "| Backpropagation | How LLMs are trained on text data |\n",
        "| Loss functions | Next-word prediction loss for language models |\n",
        "| Activation functions | Used throughout transformer architectures |\n",
        "| Hidden layers | LLMs have dozens to hundreds of layers |\n",
        "| Softmax | Converts outputs to probability over vocabulary |\n",
        "\n",
        "### Scale Comparison\n",
        "\n",
        "| Model | Parameters |\n",
        "|-------|------------|\n",
        "| Our MNIST classifier | ~109,000 |\n",
        "| GPT-2 | 1.5 billion |\n",
        "| GPT-3 | 175 billion |\n",
        "| GPT-4 | ~1.7 trillion (estimated) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeZmRqXFVRsW"
      },
      "source": [
        "---\n",
        "\n",
        "## üìù Student Exercise\n",
        "\n",
        "### Challenge: Improve the MNIST Classifier\n",
        "\n",
        "Try modifying the network architecture to improve accuracy:\n",
        "\n",
        "1. Add more hidden layers\n",
        "2. Change the number of neurons per layer\n",
        "3. Try different activation functions\n",
        "4. Add dropout for regularization\n",
        "5. Train for more epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TuSTolRVRsW"
      },
      "outputs": [],
      "source": [
        "# Student Challenge: Build an improved model\n",
        "\n",
        "improved_model = keras.Sequential([\n",
        "    layers.Input(shape=(784,)),\n",
        "\n",
        "    # TODO: Add your layers here\n",
        "    # Try: More layers, different sizes, dropout, etc.\n",
        "    # Example:\n",
        "    # layers.Dense(256, activation='relu'),\n",
        "    # layers.Dropout(0.3),  # Regularization\n",
        "    # layers.Dense(128, activation='relu'),\n",
        "\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile and train\n",
        "# improved_model.compile(...)\n",
        "# improved_model.fit(...)\n",
        "\n",
        "print(\"Complete the improved model above!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twjaBZ2FVRsW"
      },
      "source": [
        "---\n",
        "\n",
        "## üéØ Key Takeaways\n",
        "\n",
        "1. **Deep learning** automatically learns features from raw data through multiple layers\n",
        "2. **Neurons** compute weighted sums and apply activation functions\n",
        "3. **Training** involves forward pass ‚Üí loss ‚Üí backpropagation ‚Üí weight update\n",
        "4. **Gradient descent** optimizes weights by following the slope of the loss\n",
        "5. Modern **LLMs are massive neural networks** built on these same principles\n",
        "\n",
        "---\n",
        "\n",
        "### Next Module: Overview of Generative AI ‚Üí\n",
        "We'll explore autoencoders, VAEs, and the foundations of generative models!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}