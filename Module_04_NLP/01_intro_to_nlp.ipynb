{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "learning_objectives_cell",
      "metadata": {
        "id": "learning_objectives_cell"
      },
      "source": [
        "# Introduction to Natural Language Processing\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "\n",
        "- **Understand tokenization** and apply different tokenization approaches (simple, NLTK, spaCy)\n",
        "- **Implement Bag of Words (BoW)** representation from scratch and using scikit-learn\n",
        "- **Recognize BoW limitations** and understand why word order matters\n",
        "- **Understand word embeddings** and how Word2Vec captures semantic relationships\n",
        "- **Train custom Word2Vec models** and use pre-trained embeddings\n",
        "- **Compare document similarity** using cosine similarity with word embeddings\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e741da05",
      "metadata": {
        "id": "e741da05"
      },
      "source": [
        "# Basic Tokenization\n",
        "\n",
        "Tokenization in natural language processing (NLP) is the process of dividing text into smaller, meaningful units known as tokens. These tokens can be words, subwords, or even individual characters, depending on the task and language. Tokenization is typically one of the first and most important steps in preparing text for machine learning or other computational analysis because it transforms raw, unstructured text into a format that algorithms can more easily process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb4d463d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb4d463d",
        "outputId": "94255c74-015e-4ab5-de80-f28285d0cea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original sentence: NLP is a fascinating field of study.\n",
            "Tokens: ['nlp', 'is', 'a', 'fascinating', 'field', 'of', 'study.']\n"
          ]
        }
      ],
      "source": [
        "# Our sample sentence\n",
        "sentence = \"NLP is a fascinating field of study.\"\n",
        "print(f\"Original sentence: {sentence}\")\n",
        "\n",
        "# A very simple tokenizer: convert to lowercase and split by spaces\n",
        "tokens = sentence.lower().split(' ')\n",
        "\n",
        "print(f\"Tokens: {tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "943e7ff5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "943e7ff5",
        "outputId": "3200b18b-5de9-4b13-8a4e-aa2bc304fd7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"don't\", 'you', 'love', 'nlp?', \"it's\", 'a', 'fascinating', 'field!']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "sentence = \"Don't you love NLP? It's a fascinating field!\"\n",
        "\n",
        "# Tokenize using the split() method\n",
        "tokens = sentence.lower().split(' ')\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "671c14c5",
      "metadata": {
        "id": "671c14c5"
      },
      "source": [
        "Notice the problems. The punctuation is still attached to the words (`\"nlp?\"`, `\"field!\"`), and contractions like `\"don't\"` and `\"it's\"` are treated as single, unchangeable units. This method is fast but not very smart."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e12d027",
      "metadata": {
        "id": "2e12d027"
      },
      "source": [
        "## Using NLTK\n",
        "\n",
        "The __Natural Language Toolkit (NLTK)__ is a foundational library for NLP education and research. Its `word_tokenize` function is trained to handle many edge cases, like punctuation and contractions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f2b7e90",
      "metadata": {
        "id": "0f2b7e90"
      },
      "outputs": [],
      "source": [
        "!pip install nltk -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0b123ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0b123ad",
        "outputId": "c7126979-65fc-4123-c3fa-fc549ceae6f3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "262c1bd1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "262c1bd1",
        "outputId": "4544922c-fe2a-4327-dff3-b693c94acee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Do', \"n't\", 'you', 'love', 'NLP', '?', 'It', \"'s\", 'a', 'fascinating', 'field', '!']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "sentence = \"Don't you love NLP? It's a fascinating field!\"\n",
        "\n",
        "# Tokenize using NLTK's word_tokenize\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dc11e03",
      "metadata": {
        "id": "2dc11e03"
      },
      "source": [
        "NLTK correctly separates punctuation (`?`, `!`) from the words. It also intelligently splits the contraction `\"Don't\"` into its components `Do` and `n't`, which is crucial for understanding the sentence's components."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e021ad74",
      "metadata": {
        "id": "e021ad74"
      },
      "source": [
        "## spaCy (A Modern, Production-Ready Library)\n",
        "spaCy is a modern, high-performance NLP library designed for real-world applications. Its tokenizer is fast and efficient, and it's part of a larger pipeline that creates rich document objects, where each token has useful linguistic annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37e5cf0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37e5cf0b",
        "outputId": "186b6880-7d8b-40f1-d9d0-b7763960d3e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy -q\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c213ade8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c213ade8",
        "outputId": "ae9909d4-4f36-4079-c0f4-a16deed7ac9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Do', \"n't\", 'you', 'love', 'NLP', '?', 'It', \"'s\", 'a', 'fascinating', 'field', '!']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the small English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "sentence = \"Don't you love NLP? It's a fascinating field!\"\n",
        "\n",
        "# Process the sentence with the spaCy pipeline\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# The 'doc' object is a sequence of tokens. We can extract their text.\n",
        "tokens = [token.text for token in doc]\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f23b2fd",
      "metadata": {
        "id": "2f23b2fd"
      },
      "source": [
        "# Bag-of-Words from Stratch\n",
        "\n",
        "Bag of Words (BoW) is a foundational model in natural language processing for representing text data. In this approach, a text (such as a sentence or document) is converted into a collection of its words, where each unique word is treated as an individual feature, and the frequency of each word is recorded.\n",
        "\n",
        "BoW ignores the order and grammar of words, focusing solely on the presence or frequency of words within the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66614466",
      "metadata": {
        "id": "66614466"
      },
      "outputs": [],
      "source": [
        "# Our corpus of documents\n",
        "corpus = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The dog chased the cat.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "844ebdf2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "844ebdf2",
        "outputId": "22b1e260-21a9-475e-e3ad-780125c86c8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Step 1: Building the Vocabulary ---\n"
          ]
        }
      ],
      "source": [
        "# --- Step 1: Build the Vocabulary ---\n",
        "# We'll collect all unique words from the corpus in a single set\n",
        "print(\"--- Step 1: Building the Vocabulary ---\")\n",
        "all_words = set()\n",
        "for sentence in corpus:\n",
        "    # simple tokenization: lowercase and split by space\n",
        "    tokens = sentence.lower().replace('.', '').split(' ')\n",
        "    for word in tokens:\n",
        "        all_words.add(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff4ffca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bff4ffca",
        "outputId": "1b86438a-594d-4690-d51a-dee2453bd0ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'cat', 'chased', 'dog', 'mat', 'on', 'sat', 'the'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5ef326c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5ef326c",
        "outputId": "149a28c3-5f8a-4f87-fdba-08700a60e7b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Vocabulary: ['cat', 'chased', 'dog', 'mat', 'on', 'sat', 'the']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Convert the set to a sorted list to have a consistent order\n",
        "vocabulary = sorted(list(all_words))\n",
        "print(f\"Final Vocabulary: {vocabulary}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09884d8f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09884d8f",
        "outputId": "fcf65f1b-b83f-42d5-f9df-83606bfc280d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Step 2: Creating the Vectors ---\n"
          ]
        }
      ],
      "source": [
        "# --- Step 2: Create the Vectors ---\n",
        "# We'll create a vector for each sentence by counting word occurrences\n",
        "print(\"--- Step 2: Creating the Vectors ---\")\n",
        "final_vectors = []\n",
        "for sentence in corpus:\n",
        "    # Start with a vector of zeros, one position for each word in the vocabulary\n",
        "    vector = [0] * len(vocabulary)\n",
        "\n",
        "    # Tokenize the current sentence\n",
        "    sentence_tokens = sentence.lower().replace('.', '').split(' ')\n",
        "\n",
        "    # Count the words\n",
        "    for word in sentence_tokens:\n",
        "        # Find the index of the word in our vocabulary\n",
        "        if word in vocabulary:\n",
        "            index = vocabulary.index(word)\n",
        "            # Increment the count at that index\n",
        "            vector[index] += 1\n",
        "\n",
        "    final_vectors.append(vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19337c7f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19337c7f",
        "outputId": "0f9e0371-91dc-4064-b3e2-8de26c8ff258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1 Vector: [1, 0, 0, 1, 1, 1, 2]\n",
            "Sentence 2 Vector: [1, 1, 1, 0, 0, 0, 2]\n"
          ]
        }
      ],
      "source": [
        "# Print the results beautifully\n",
        "print(\"Sentence 1 Vector:\", final_vectors[0])\n",
        "print(\"Sentence 2 Vector:\", final_vectors[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9295af5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9295af5",
        "outputId": "a7a6da5b-c7e1-42cb-a509-dd70b1a4158d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Readable DataFrame ---\n",
            "            cat  chased  dog  mat  on  sat  the\n",
            "Sentence 1    1       0    0    1   1    1    2\n",
            "Sentence 2    1       1    1    0   0    0    2\n"
          ]
        }
      ],
      "source": [
        "# For comparison with the scikit-learn output\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(final_vectors, columns=vocabulary, index=['Sentence 1', 'Sentence 2'])\n",
        "print(\"\\n--- Readable DataFrame ---\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df6a0149",
      "metadata": {
        "id": "df6a0149"
      },
      "source": [
        "# Bag-of-Words with Scikit-Learn\n",
        "\n",
        "__CountVectorizer__ from the scikit-learn library is commonly used to implement the Bag of Words (BoW) model. It converts a collection of text documents into a matrix of token counts, where each row represents a document and each column represents a unique word (token) from the entire corpus vocabulary. The values in the matrix indicate the frequency of each word in each document.\n",
        "\n",
        "CountVectorizer handles tokenization, lowercasing, and counting word occurrences automatically, making it a practical and efficient tool for generating BoW representations for text data. For example, for a corpus of documents, CountVectorizer creates a sparse matrix of word counts that can be used directly as input features for machine learning models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b77c051",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b77c051",
        "outputId": "bed9b9e5-63cf-4832-b39a-14da4908c492"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learned Vocabulary: ['cat' 'chased' 'dog' 'mat' 'on' 'sat' 'the']\n"
          ]
        }
      ],
      "source": [
        "# Import the CountVectorizer class from scikit-learn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Our corpus of documents\n",
        "corpus = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The dog chased the cat.\"\n",
        "]\n",
        "\n",
        "# 1. Create an instance of CountVectorizer\n",
        "# This object will learn the vocabulary and generate vectors\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# 2. Fit the vectorizer to the corpus and transform the corpus into vectors\n",
        "# .fit_transform() learns the vocabulary and returns the document-term matrix (our vectors)\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# 3. Get the learned vocabulary\n",
        "# The vocabulary is a dictionary where keys are words and values are their index positions in the vector\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "print(f\"Learned Vocabulary: {vocabulary}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05a84800",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05a84800",
        "outputId": "03343182-742b-437d-f406-dd3d43cfc0f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Resulting Vectors (Document-Term Matrix):\n",
            "[[1 0 0 1 1 1 2]\n",
            " [1 1 1 0 0 0 2]]\n"
          ]
        }
      ],
      "source": [
        "# 4. View the vectors\n",
        "# The result is a sparse matrix. We convert it to a dense array for readability.\n",
        "vectors = X.toarray()\n",
        "print(\"\\nResulting Vectors (Document-Term Matrix):\")\n",
        "print(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a84f8f38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a84f8f38",
        "outputId": "f3a77bd4-ff94-4a79-8d3d-739d9199df11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Readable DataFrame ---\n",
            "            cat  chased  dog  mat  on  sat  the\n",
            "Sentence 1    1       0    0    1   1    1    2\n",
            "Sentence 2    1       1    1    0   0    0    2\n"
          ]
        }
      ],
      "source": [
        "# For a more readable output, let's put it in a pandas DataFrame\n",
        "df = pd.DataFrame(vectors, columns=vocabulary, index=['Sentence 1', 'Sentence 2'])\n",
        "print(\"\\n--- Readable DataFrame ---\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77cc7447",
      "metadata": {
        "id": "77cc7447"
      },
      "source": [
        "#  BoW Limitation (Loss of Word Order)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90b92727",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90b92727",
        "outputId": "054e92e9-1685-4461-9d90-4798141e086b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Vocabulary and Vectors ---\n",
            "            bit  dog  man  the\n",
            "Sentence A    1    1    1    2\n",
            "Sentence B    1    1    1    2\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Two sentences with completely different meanings\n",
        "sentences_with_different_meanings = [\n",
        "    \"The dog bit the man.\",\n",
        "    \"The man bit the dog.\"\n",
        "]\n",
        "\n",
        "# Use the same CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(sentences_with_different_meanings)\n",
        "\n",
        "# Display the results\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "df = pd.DataFrame(X.toarray(), columns=vocabulary, index=['Sentence A', 'Sentence B'])\n",
        "\n",
        "print(\"--- Vocabulary and Vectors ---\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e36b51f4",
      "metadata": {
        "id": "e36b51f4"
      },
      "source": [
        "Although the sentences mean opposite things, their Bag-of-Words representations are identical. This proves that the model has lost the crucial information contained in the word order."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdce7422",
      "metadata": {
        "id": "cdce7422"
      },
      "source": [
        "# Word2Vec\n",
        "\n",
        "Word2Vec is a neural network-based technique created by Google researchers in 2013 to learn dense vector representations of words called word embeddings. Unlike Bag of Words (BoW), which represents text by counting word frequencies and ignores word order, Word2Vec captures semantic relationships and contextual meaning between words by training on word co-occurrence in a corpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "883466d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "883466d1",
        "outputId": "d70d8f0c-4374-4e7a-cda8-31a4bea2c74e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install gensim -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1WD4kqGYOpU",
      "metadata": {
        "id": "d1WD4kqGYOpU"
      },
      "source": [
        "## Training Your Own Mini Word2Vec Model\n",
        "\n",
        " __Gensim__ offers an efficient and straightforward API to train Word2Vec models on custom text corpora. You provide the model with preprocessed tokenized sentences, and it learns word embeddings using either the CBOW or Skip-Gram method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ca641fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ca641fc",
        "outputId": "0c6755ec-2d70-42f7-aea5-0d0d873567de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt') # Download tokenizer data\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b74bdd9",
      "metadata": {
        "id": "7b74bdd9"
      },
      "outputs": [],
      "source": [
        "# 1. Sample Corpus\n",
        "corpus = [\n",
        "    \"Generative AI is a powerful technology.\",\n",
        "    \"Large language models can create human-like text.\",\n",
        "    \"The transformer architecture revolutionized NLP.\",\n",
        "    \"Deep learning models require significant data.\",\n",
        "    \"Artificial intelligence is a broad field of study.\",\n",
        "    \"Transformers are the basis for models like GPT.\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qtw1XhFCYbaj",
      "metadata": {
        "id": "Qtw1XhFCYbaj"
      },
      "source": [
        "Here's a brief outline of how you can train Word2Vec with Gensim:\n",
        "\n",
        "* Prepare your text data as tokenized sentences (lists of words).\n",
        "\n",
        "* Initialize the Word2Vec model with parameters such as vector size, window size, minimum word count, and training algorithm.\n",
        "\n",
        "* Train the model on your sentences.\n",
        "\n",
        "* Save the trained model for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29634524",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29634524",
        "outputId": "15962567-385b-40ad-8fe7-7df7f3725639"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['generative', 'ai', 'is', 'a', 'powerful', 'technology', '.'],\n",
              " ['large', 'language', 'models', 'can', 'create', 'human-like', 'text', '.'],\n",
              " ['the', 'transformer', 'architecture', 'revolutionized', 'nlp', '.'],\n",
              " ['deep', 'learning', 'models', 'require', 'significant', 'data', '.'],\n",
              " ['artificial',\n",
              "  'intelligence',\n",
              "  'is',\n",
              "  'a',\n",
              "  'broad',\n",
              "  'field',\n",
              "  'of',\n",
              "  'study',\n",
              "  '.'],\n",
              " ['transformers', 'are', 'the', 'basis', 'for', 'models', 'like', 'gpt', '.']]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2. Preprocess Data (Tokenization)\n",
        "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
        "tokenized_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc11d4ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc11d4ea",
        "outputId": "e6fcb45e-d1e9-4797-9d71-119db2d28419"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(101, 460)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3. Train Word2Vec Model\n",
        "# vector_size: dimensionality of the word vectors\n",
        "# window: max distance between the current and predicted word within a sentence\n",
        "# min_count: ignores all words with total frequency lower than this\n",
        "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0205a0cc",
      "metadata": {
        "id": "0205a0cc"
      },
      "outputs": [],
      "source": [
        "# 4. Create Document Vectors\n",
        "# We'll use a simple approach: average the word vectors for each document\n",
        "def get_doc_vector(doc_tokens, model):\n",
        "    word_vectors = [model.wv[word] for word in doc_tokens if word in model.wv]\n",
        "    if not word_vectors:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "doc_vectors = [get_doc_vector(doc, model) for doc in tokenized_corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7aa74f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7aa74f7",
        "outputId": "1e3c7255-8551-4236-874a-23d2ac4b2b90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Document: 'Generative AI is a powerful technology.'\n",
            "Most Similar Document: 'Artificial intelligence is a broad field of study.'\n",
            "Similarity Score: 0.3231\n"
          ]
        }
      ],
      "source": [
        "# --- Step 5: Calculate and Find Most Similar Document (scikit-learn approach) ---\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Let's find documents similar to the first one\n",
        "query_vector = doc_vectors[0]\n",
        "\n",
        "# scikit-learn's function expects 2D arrays, so we reshape the query vector\n",
        "# The output is a 2D array, so we access the first (and only) row with [0]\n",
        "similarities = cosine_similarity(query_vector.reshape(1, -1), doc_vectors)[0]\n",
        "\n",
        "# Find the most similar document (excluding itself)\n",
        "most_similar_idx = np.argsort(similarities)[-2] # -1 is the document itself\n",
        "\n",
        "print(f\"Original Document: '{corpus[0]}'\")\n",
        "print(f\"Most Similar Document: '{corpus[most_similar_idx]}'\")\n",
        "print(f\"Similarity Score: {similarities[most_similar_idx]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "216ca531",
      "metadata": {
        "id": "216ca531"
      },
      "source": [
        "## Pre-trained Word2Vec\n",
        "In this lab, we'll use a pre-trained Word2Vec model to find the most similar document to a given query. Instead of training a model ourselves (which takes a lot of data and time), we'll use a model trained on a massive dataset (like all of Wikipedia).\n",
        "\n",
        "Our strategy will be to represent each document by taking the average of the word vectors of all the words within it. This gives us a single vector that captures the document's overall meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e92a2d3",
      "metadata": {
        "id": "3e92a2d3"
      },
      "outputs": [],
      "source": [
        "# --- Step 1: Setup and Load the Pre-trained Model ---\n",
        "# We use gensim's downloader to fetch a pre-trained model.\n",
        "# 'glove-wiki-gigaword-50' is a small model with 50-dimensional vectors.\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZV8sNzKsamdo",
      "metadata": {
        "id": "ZV8sNzKsamdo"
      },
      "source": [
        "The model `glove-wiki-gigaword-50` is a pretrained word embedding model based on the GloVe (Global Vectors for Word Representation) algorithm developed by Stanford NLP. It provides 50-dimensional dense vector representations for words, trained on a large combined corpus including Wikipedia (2014 dump) and the Gigaword dataset, containing billions of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R5hBCvOzZenQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5hBCvOzZenQ",
        "outputId": "a2a2df55-b383-414e-e5b0-4dfc61d73c57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ],
      "source": [
        "model = api.load(\"glove-wiki-gigaword-50\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e160a66",
      "metadata": {
        "id": "1e160a66"
      },
      "outputs": [],
      "source": [
        "# --- Step 2: Define Our Documents ---\n",
        "# We have three documents with different topics.\n",
        "documents = [\n",
        "    \"The sun is the star at the center of the Solar System.\", # About space\n",
        "    \"The ocean is a body of salt water that covers most of the Earth.\", # About oceans\n",
        "    \"A computer is a machine that can be programmed to carry out sequences of arithmetic or logical operations.\" # About technology\n",
        "]\n",
        "doc_labels = [\"Space Document\", \"Ocean Document\", \"Technology Document\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "497ddb84",
      "metadata": {
        "id": "497ddb84"
      },
      "outputs": [],
      "source": [
        "# --- Step 3: Create a Function to Vectorize a Document ---\n",
        "# This function converts a document into a single vector by averaging its word vectors.\n",
        "def vectorize_document(doc, model):\n",
        "    \"\"\"Converts a document string into a single averaged vector.\"\"\"\n",
        "    words = doc.lower().split()\n",
        "\n",
        "    # Get the vector for each word in the document, if the word exists in the model\n",
        "    word_vectors = [model[word] for word in words if word in model]\n",
        "\n",
        "    if not word_vectors:\n",
        "        # If no words in the document are in the model's vocabulary, return a vector of zeros\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "    # Return the mean of the word vectors to get a single document vector\n",
        "    return np.mean(word_vectors, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0001055",
      "metadata": {
        "id": "c0001055"
      },
      "outputs": [],
      "source": [
        "# --- Step 4: Vectorize All Our Documents ---\n",
        "doc_vectors = [vectorize_document(doc, model) for doc in documents]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5104003",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5104003",
        "outputId": "231ab6d4-a6d0-4c7f-8ba5-f1fe51b1d982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: 'The astronaut travels to the moon in a rocket.'\n"
          ]
        }
      ],
      "source": [
        "# --- Step 5: Define a Query and Find the Most Similar Document ---\n",
        "query = \"The astronaut travels to the moon in a rocket.\"\n",
        "print(f\"\\nQuery: '{query}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8da91e6",
      "metadata": {
        "id": "c8da91e6"
      },
      "outputs": [],
      "source": [
        "# Vectorize the query using the same function\n",
        "query_vector = vectorize_document(query, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cc84249",
      "metadata": {
        "id": "4cc84249"
      },
      "outputs": [],
      "source": [
        "# Calculate cosine similarity between the query vector and all document vectors\n",
        "# We need to reshape the vectors for the function to work correctly.\n",
        "similarities = cosine_similarity(query_vector.reshape(1, -1), doc_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0d44f8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0d44f8d",
        "outputId": "bc977ae0-b068-433e-e74f-8657626eb5d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Similarity Scores: [0.9492896  0.90947753 0.8427572 ]\n",
            "The most similar document is: 'Space Document'\n",
            "Document Text: 'The sun is the star at the center of the Solar System.'\n"
          ]
        }
      ],
      "source": [
        "# Find the index of the most similar document\n",
        "most_similar_index = np.argmax(similarities)\n",
        "\n",
        "print(f\"\\nSimilarity Scores: {similarities[0]}\")\n",
        "print(f\"The most similar document is: '{doc_labels[most_similar_index]}'\")\n",
        "print(f\"Document Text: '{documents[most_similar_index]}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0vw3BEDU7oTL",
      "metadata": {
        "id": "0vw3BEDU7oTL"
      },
      "source": [
        "### Assignment: Word2Vec with a Larger Corpus\n",
        "\n",
        "Your task is to repeat the Word2Vec model training process on a larger corpus of your choice.\n",
        "\n",
        "1.  **Find a Corpus**: Find a text file (`.txt`) to use as your corpus. You can use sources like [Project Gutenberg](https://www.gutenberg.org/) to find books in plain text format. Download a book and save it in a `Resources` folder.\n",
        "2.  **Load and Preprocess**: Load the text data and preprocess it similar to the examples above (e.g., tokenization, lowercasing).\n",
        "3.  **Train the Model**: Train a `Word2Vec` model on your corpus. You might need to experiment with the model parameters (`vector_size`, `window`, `min_count`, etc.) to get good results.\n",
        "4.  **Explore the Embeddings**:\n",
        "    *   Find the most similar words for a few words in your vocabulary.\n",
        "    *   Perform some word analogies (e.g., \"king\" - \"man\" + \"woman\" = \"queen\").\n",
        "    *   Implement a function to find the most similar document (sentence) in your corpus for a given query sentence. Test it with a few queries.\n",
        "5.  **Reflect**: Briefly describe your findings. Are the results better or worse than the small example? What did you learn?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C3rbErSR7tjv",
      "metadata": {
        "id": "C3rbErSR7tjv"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "file_id = '13ETSh6QARlutXhFePwQaqVgZ4TYbRd2o'\n",
        "output = 'corpus.txt'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dca0a5ea",
      "metadata": {
        "id": "dca0a5ea"
      },
      "outputs": [],
      "source": [
        "# Load the corpus\n",
        "with open('corpus.txt', 'r') as f:\n",
        "    corpus = f.readlines()\n",
        "\n",
        "# Preprocess the data\n",
        "\n",
        "\n",
        "# Train the Word2Vec model\n",
        "\n",
        "\n",
        "# Explore the embeddings\n",
        "# Find the most similar words to 'king'\n",
        "model.wv.most_similar('king')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RD3J3sUC8HLr",
      "metadata": {
        "id": "RD3J3sUC8HLr"
      },
      "outputs": [],
      "source": [
        "# Create document vectors for the entire corpus by averaging word vectors\n",
        "\n",
        "doc_vectors ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kjL3YUKH8SE7",
      "metadata": {
        "id": "kjL3YUKH8SE7"
      },
      "outputs": [],
      "source": [
        "# Define a query sentence\n",
        "query = \"a rocket to the moon\"\n",
        "\n",
        "# Vectorize the query using the correct function and tokenizing it\n",
        "query_tokens =\n",
        "query_vector =\n",
        "\n",
        "# scikit-learn's function expects 2D arrays, so we reshape the query vector\n",
        "# The output is a 2D array, so we access the first (and only) row with [0]\n",
        "similarities =\n",
        "\n",
        "# Find the most similar document (excluding itself)\n",
        "most_similar_idx =\n",
        "\n",
        "print(f\"Original Document: '{query}'\")\n",
        "print(f\"Most Similar Document: '{corpus[most_similar_idx]}'\")\n",
        "print(f\"Similarity Score: {similarities[most_similar_idx]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mph_v3BT8XIn",
      "metadata": {
        "id": "mph_v3BT8XIn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}