{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbBeFpHvMsOm"
      },
      "source": [
        "# Introduction to LLM APIs: OpenAI and Ollama\n",
        "\n",
        "## Overview\n",
        "In this notebook, we will explore how to interact with Large Language Models (LLMs) using Python.\n",
        "We will cover two main approaches:\n",
        "\n",
        "1.  **Proprietary Models (OpenAI)**: Using the OpenAI API to access powerful models like GPT-4o.\n",
        "2.  **Open Source Models (Ollama)**: Running local LLMs (like Llama 3 or Phi-4) on your own machine (or in this case, the Google Colab environment).\n",
        "\n",
        "By the end of this session, you will understand how to send prompts to these models and receive responses programmatically.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci-RlkmaMsOn"
      },
      "source": [
        "## 1. OpenAI Python API\n",
        "\n",
        "First, we need to install the official OpenAI Python SDK. This library simplifies making requests to OpenAI's servers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXNzOEZhMsOn"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq openai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMzHjlhXMsOn"
      },
      "source": [
        "### API Key Setup\n",
        "\n",
        "To use OpenAI, you need an API key. In Google Colab, it is best practice to store your keys in the `Secrets` manager (the key icon on the left sidebar).\n",
        "\n",
        "1.  Click the **key icon** on the left.\n",
        "2.  Add a new secret named `OPENAI_API_KEY` with your actual key value.\n",
        "3.  Toggle 'Notebook access' to on.\n",
        "\n",
        "The code below retrieves this key securely.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl47IUjtMsOn"
      },
      "source": [
        "**Run the following cell if you are running this from inside Google Colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AAsDKPeMsOn"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    api_key = userdata.get('OPENAI_API_KEY')\n",
        "except Exception as e:\n",
        "    print(\"Error retrieving API key. Make sure you set 'OPENAI_API_KEY' in Colab Secrets.\")\n",
        "    api_key = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRqpRuiLMsOn"
      },
      "source": [
        "**Run the following if you are using Colab from within VSCode or just using VSCode**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ3N7HU6MsOo"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import openai\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv()\n",
        "\n",
        "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# if api_key is None:\n",
        "#     raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmXGBNKIMsOo"
      },
      "source": [
        "### Basic Completion\n",
        "\n",
        "Let's make our first call to the API. We will use the `chat.completions.create` method.\n",
        "This method requires:\n",
        "-   `model`: The specific model ID (e.g., `gpt-4o-mini`).\n",
        "-   `messages`: A list of message objects, where each object has a `role` (system, user, assistant) and `content`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zjeaYZNMsOo",
        "outputId": "548ee57a-5c27-44b1-d275-d046eaaee03d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A Large Language Model is an advanced artificial intelligence system designed to understand, generate, and manipulate human language by utilizing vast amounts of text data and sophisticated algorithms to predict and produce coherent and contextually relevant language outputs.\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Explain what a Large Language Model is in one sentence.\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "response = completion.choices[0].message.content\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UMfXGf8MsOo",
        "outputId": "a9059bce-8ad0-4308-d002-30cce41ef35a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A Large Language Model (LLM) is a type of artificial intelligence designed to understand, generate, and manipulate human language. These models are built using deep learning techniques, particularly neural networks, and trained on vast amounts of text data. Here‚Äôs a deep dive into the components, workings, and implications of LLMs:\n",
            "\n",
            "### 1. **Architecture**\n",
            "LLMs are typically based on the transformer architecture, which was introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. The key components of this architecture include:\n",
            "\n",
            "- **Attention Mechanism**: This allows the model to weigh the importance of different words in a sentence when processing language. Instead of processing words sequentially, the attention mechanism enables the model to consider all words simultaneously, improving context understanding.\n",
            "\n",
            "- **Self-Attention**: Within the transformer, self-attention computes representations of the input by relating different positions of the input sequence to each other. This is crucial for capturing context and relationships in language.\n",
            "\n",
            "- **Multi-Head Attention**: The model uses multiple attention heads to capture different types of relationships and contexts simultaneously. Each head can focus on different parts of the input, enriching the model's understanding.\n",
            "\n",
            "- **Feedforward Neural Networks**: After the attention layers, the output is passed through feedforward neural networks, which apply additional transformations to the data.\n",
            "\n",
            "- **Positional Encoding**: Since transformers do not process data in sequence, positional encodings are added to the input embeddings to give the model information about the order of words.\n",
            "\n",
            "### 2. **Training Process**\n",
            "LLMs are trained using a method called unsupervised learning, where they learn from large datasets of text without explicit labels. The training process typically involves:\n",
            "\n",
            "- **Data Collection**: LLMs are trained on diverse and extensive datasets, which can include books, articles, websites, and other text sources.\n",
            "\n",
            "- **Tokenization**: The text is broken down into smaller units called tokens (words, subwords, or characters) which the model uses for processing.\n",
            "\n",
            "- **Objective Function**: Most LLMs are trained to predict the next token in a sequence given the previous tokens. This is known as language modeling and is typically framed as minimizing the cross-entropy loss between the predicted and actual next tokens.\n",
            "\n",
            "- **Fine-tuning**: After pre-training on a broad corpus, LLMs can be fine-tuned on specific tasks (like question answering or summarization) using\n"
          ]
        }
      ],
      "source": [
        "# using temperature, max_tokens\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Explain what a Large Language Model is in depth.\"}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=500,\n",
        "        )\n",
        "\n",
        "response = completion.choices[0].message.content\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJiH9paeMsOo",
        "outputId": "65110a41-bec6-4b4a-d0cd-ce98486d7bf2"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "A Large Language Model (LLM) is a type of artificial intelligence designed to understand, generate, and manipulate human language. These models are built using deep learning techniques, particularly neural networks, and trained on vast amounts of text data. Here‚Äôs a deep dive into the components, workings, and implications of LLMs:\n",
              "\n",
              "### 1. **Architecture**\n",
              "LLMs are typically based on the transformer architecture, which was introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. The key components of this architecture include:\n",
              "\n",
              "- **Attention Mechanism**: This allows the model to weigh the importance of different words in a sentence when processing language. Instead of processing words sequentially, the attention mechanism enables the model to consider all words simultaneously, improving context understanding.\n",
              "\n",
              "- **Self-Attention**: Within the transformer, self-attention computes representations of the input by relating different positions of the input sequence to each other. This is crucial for capturing context and relationships in language.\n",
              "\n",
              "- **Multi-Head Attention**: The model uses multiple attention heads to capture different types of relationships and contexts simultaneously. Each head can focus on different parts of the input, enriching the model's understanding.\n",
              "\n",
              "- **Feedforward Neural Networks**: After the attention layers, the output is passed through feedforward neural networks, which apply additional transformations to the data.\n",
              "\n",
              "- **Positional Encoding**: Since transformers do not process data in sequence, positional encodings are added to the input embeddings to give the model information about the order of words.\n",
              "\n",
              "### 2. **Training Process**\n",
              "LLMs are trained using a method called unsupervised learning, where they learn from large datasets of text without explicit labels. The training process typically involves:\n",
              "\n",
              "- **Data Collection**: LLMs are trained on diverse and extensive datasets, which can include books, articles, websites, and other text sources.\n",
              "\n",
              "- **Tokenization**: The text is broken down into smaller units called tokens (words, subwords, or characters) which the model uses for processing.\n",
              "\n",
              "- **Objective Function**: Most LLMs are trained to predict the next token in a sequence given the previous tokens. This is known as language modeling and is typically framed as minimizing the cross-entropy loss between the predicted and actual next tokens.\n",
              "\n",
              "- **Fine-tuning**: After pre-training on a broad corpus, LLMs can be fine-tuned on specific tasks (like question answering or summarization) using"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# improving output format using Markdown\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "Markdown(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oDrlBPpMsOo"
      },
      "source": [
        "### Streaming Responses\n",
        "\n",
        "LLMs generate text token by token. Instead of waiting for the full response, we can 'stream' the output so it appears as it is being written. This creates a better user experience.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSZAWVwhMsOo",
        "outputId": "bf2707f4-cc8c-4d53-8c8e-a532e0624dea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming response:\n",
            "Okay, imagine you have a toy robot that wants to learn how to recognize different animals. \n",
            "\n",
            "First, we give the robot lots and lots of pictures of animals, like cats, dogs, and rabbits. Each picture is like a puzzle piece. The robot has a special brain called a \"neural network,\" which is made up of tiny helpers, kind of like a team of little friends.\n",
            "\n",
            "When the robot looks at a picture, each little helper takes a tiny look at just a part of the picture. Some helpers look at colors, some look at shapes, and others look at where things are in the picture.\n",
            "\n",
            "The helpers then talk to each other and pass their thoughts along, trying to guess what animal it is. If they get it right, they cheer! If they get it wrong, they remember what they learned and try to do better next time.\n",
            "\n",
            "Over and over, the robot looks at new pictures, and with each picture, the little helpers get better at guessing! So, after a lot of practice, the robot becomes really good at recognizing animals, just like how you get better at games the more you play them!"
          ]
        }
      ],
      "source": [
        "stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \"Explain how a neural network works to a 5-year-old.\"}\n",
        "        ],\n",
        "        stream=True,  # Enable streaming\n",
        "    )\n",
        "\n",
        "print(\"Streaming response:\")\n",
        "for chunk in stream:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content:\n",
        "        print(content, end=\"\", flush=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wroHw3RtMsOo",
        "outputId": "fad343f0-c28f-4c01-9563-7f42dab89fd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming markdown response:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Large language models (LLMs) are a type of artificial intelligence model designed to understand, generate, and manipulate human language. These models are built using deep learning techniques, particularly neural networks, and are trained on vast amounts of text data to learn the patterns, structure, and nuances of language. Here are some key characteristics and features of LLMs:\n",
              "\n",
              "1. **Scale**: LLMs are typically characterized by their size, which is often measured in terms of the number of parameters they have. Parameters are the internal variables that the model learns during training. A larger number of parameters generally allows the model to capture more complex patterns and relationships in language.\n",
              "\n",
              "2. **Training Data**: LLMs are trained on diverse datasets that include books, articles, websites, and other written content. The training process involves the model learning to predict the next word in a sentence given the preceding words, enabling it to pick up grammar, facts, and some contextual understanding.\n",
              "\n",
              "3. **Versatility**: Once trained, LLMs can perform a wide range of natural language processing (NLP) tasks, including text generation, translation, summarization, question answering, and sentiment analysis, among others. Their generalization capabilities allow them to handle a variety of topics and writing styles.\n",
              "\n",
              "4. **Contextual Understanding**: LLMs are capable of understanding context to a significant extent, which allows them to generate responses that are coherent and contextually appropriate. This is often achieved through mechanisms like attention, which helps the model focus on relevant parts of the input text while generating output.\n",
              "\n",
              "5. **Zero-shot and Few-shot Learning**: Some LLMs can perform tasks with little or no task-specific training. This means they can generalize their knowledge to apply to new tasks, making them flexible and powerful for various applications without extensive retraining.\n",
              "\n",
              "6. **Challenges**: Despite their capabilities, LLMs have limitations. They can produce biased or factually incorrect outputs, lack true understanding or common sense reasoning, and may generate nonsensical or irrelevant text. Additionally, issues related to ethical use, data privacy, and misinformation are ongoing concerns in the deployment of LLMs.\n",
              "\n",
              "7. **Examples**: Some well-known large language models include OpenAI's GPT-3 and GPT-4, Google's BERT, and Facebook's LLaMA. Each of these models has its unique architecture and training framework but shares the common goal of advancing the understanding and generation of human language.\n",
              "\n",
              "In summary, large language models represent a significant advancement in natural language processing and artificial intelligence, enabling more sophisticated interactions between humans and machines through language understanding and generation. They continue to evolve, driving research and development in both academia and industry."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Markdown, clear_output\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \"Explain what are large language models.\"}\n",
        "        ],\n",
        "        stream=True,  # Enable streaming\n",
        "    )\n",
        "\n",
        "print(\"Streaming markdown response:\")\n",
        "full_response_content = \"\"\n",
        "display_handle = display(Markdown(\"\"), display_id=True)\n",
        "\n",
        "for chunk in stream:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content:\n",
        "        full_response_content += content\n",
        "        display_handle.update(Markdown(full_response_content))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLwJxhnFMsOo"
      },
      "source": [
        "## 2. Ollama (Local LLMs)\n",
        "\n",
        "[Ollama](https://ollama.com/) is a tool that allows you to run open-source LLMs locally. It simplifies the process of downloading and managing models like Llama 3, Mistral, and Gemma.\n",
        "\n",
        "### Setting up Ollama in Colab\n",
        "Since Google Colab is a virtual environment, we need to:\n",
        "1.  Install Ollama.\n",
        "2.  Start the Ollama server in the background.\n",
        "3.  Pull (download) the model we want to use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4Az8MkrMsOo"
      },
      "outputs": [],
      "source": [
        "# 1. Install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxyzth1RMsOp"
      },
      "outputs": [],
      "source": [
        "# 2. Start Ollama server in the background using nohup\n",
        "!nohup ollama serve > ollama.log 2>&1 &\n",
        "\n",
        "# Wait a few seconds for the server to spin up\n",
        "import time\n",
        "time.sleep(5)\n",
        "print(\"Ollama server started.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgC2rrh4MsOp"
      },
      "outputs": [],
      "source": [
        "# 3. Pull a lightweight model (Llama 3.2 is great for Colab)\n",
        "!ollama pull qwen3:4b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKsRoItyMsOp"
      },
      "source": [
        "### Using Ollama Python Library\n",
        "\n",
        "Just like OpenAI, Ollama has a Python library to interact with the models running on the local server.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bwmCYo_MsOp"
      },
      "outputs": [],
      "source": [
        "# Install the Ollama python client\n",
        "!uv pip install -q ollama\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGXAhtKXMsOp",
        "outputId": "30d39c15-cb40-4061-d17e-82793cc4ea38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are the **core components of a RAG (Retrieval-Augmented Generation) application**, explained clearly and concisely for both technical and non-technical audiences. I'll focus on *what* each component does and *why* it matters‚Äîavoiding excessive jargon while ensuring practical relevance.\n",
            "\n",
            "---\n",
            "\n",
            "### üß† 1. **Data Ingestion & Storage**\n",
            "- **What it does**: Collects, stores, and organizes raw data (e.g., PDFs, websites, databases, text files) into a structured format.\n",
            "- **Why it matters**: Without this, there's no data to retrieve. Real-world examples:  \n",
            "  ‚Üí *Customer support tickets* (for troubleshooting)  \n",
            "  ‚Üí *Internal knowledge bases* (for HR policies)  \n",
            "  ‚Üí *Research papers* (for academic answers)\n",
            "- **Key tools**: S3 buckets, databases (e.g., PostgreSQL), or cloud storage.\n",
            "\n",
            "---\n",
            "\n",
            "### üîç 2. **Document Preprocessing**\n",
            "- **What it does**: Cleans, splits, and transforms raw documents into a format suitable for retrieval (e.g., removing noise, splitting into chunks, standardizing text).\n",
            "- **Why it matters**: Prevents irrelevant results. Example:  \n",
            "  ‚Üí Splitting a 50-page PDF into 1000-character chunks so the model can find *exact* answers.  \n",
            "  ‚Üí Removing tables, images, or irrelevant sections from web pages.\n",
            "- **Key tools**: Python libraries like `PyPDF2`, `BeautifulSoup`, or `LangChain`'s chunkers.\n",
            "\n",
            "---\n",
            "\n",
            "### üì¶ 3. **Indexing (Vector Database)**\n",
            "- **What it does**: Converts processed documents into **vectors** (numerical representations) and stores them in a fast-searchable database.\n",
            "- **Why it matters**: This is the *heart* of RAG. Without it, you can't quickly find relevant documents.  \n",
            "  ‚Üí *Example*: When a user asks \"How do I reset my password?\", the system converts this query into a vector and searches the vector database for documents with similar vectors (e.g., \"password reset instructions\").\n",
            "- **Key tools**: FAISS, Pinecone, Weaviate, or ChromaDB.  \n",
            "  ‚Üí *Critical note*: This is where **embedding models** (e.g., `all-MiniLM-L6-v2`) live.\n",
            "\n",
            "---\n",
            "\n",
            "### üïµÔ∏è 4. **Retrieval System**\n",
            "- **What it does**: Takes a user query ‚Üí converts it into a vector ‚Üí searches the index ‚Üí returns *top-k* relevant documents.\n",
            "- **Why it matters**: This is where RAG *augments* the LLM. Without good retrieval, the LLM gets \"hallucinated\" answers (made up facts).  \n",
            "  ‚Üí *Example*: If the query is \"What is the capital of France?\", the system retrieves the document \"France: Paris is the capital\" (not a random document about \"France\" in a history book).\n",
            "- **Key metrics**: Precision (how many relevant docs?), Recall (how many relevant docs are found?).\n",
            "\n",
            "---\n",
            "\n",
            "### üß© 5. **Query Processing Pipeline**\n",
            "- **What it does**: Prepares the user query for retrieval (e.g., adding context, handling typos, language translation).\n",
            "- **Why it matters**: Ensures the query vector aligns with the index.  \n",
            "  ‚Üí *Example*: If a user types \"reset pass\", the system might correct it to \"reset password\" before retrieval to avoid mismatched results.\n",
            "\n",
            "---\n",
            "\n",
            "### ü§ñ 6. **Generation Module (LLM)**\n",
            "- **What it does**: Takes the retrieved documents + user query ‚Üí generates a **natural-language response**.\n",
            "- **Why it matters**: This is where the *augmentation* happens. The LLM uses retrieved info to answer *accurately* (e.g., \"Your password reset link is: `https://...`\").  \n",
            "  ‚Üí *Without RAG*: The LLM might hallucinate (\"Password reset links expire in 24 hours...\") because it has no recent data.  \n",
            "  ‚Üí *With RAG*: The LLM uses the retrieved document to say \"Your link expires in 1 hour\" (if the doc states this).\n",
            "- **Key tools**: LLMs like `GPT-4`, `Llama 3`, or `Claude`.\n",
            "\n",
            "---\n",
            "\n",
            "### üìä 7. **Evaluation & Feedback Loop**\n",
            "- **What it does**: Tests the RAG system's performance (e.g., accuracy, latency) and uses user feedback to improve.\n",
            "- **Why it matters**: RAG isn't a \"one-off\" solution. Real-world systems need constant tuning.  \n",
            "  ‚Üí *Example*: If users say \"This answer is wrong\", the system updates the index or retrieval model.\n",
            "- **Key metrics**:  \n",
            "  - **Precision**: % of retrieved docs that are relevant  \n",
            "  - **Relevance score**: How well the response matches the query  \n",
            "  - **Latency**: Time from query to response (critical for apps)\n",
            "\n",
            "---\n",
            "\n",
            "### üí° Why This Matters in Practice\n",
            "RAG solves a **real problem**: LLMs (like GPT) often hallucinate answers because they lack context. By *retrieving relevant documents first*, RAG:  \n",
            "‚úÖ **Reduces hallucinations** (uses actual data)  \n",
            "‚úÖ **Improves accuracy** (answers are grounded in your knowledge base)  \n",
            "‚úÖ **Scales better** (works with large datasets without retraining the LLM)\n",
            "\n",
            "> üåü **Real-world example**: A bank‚Äôs chatbot uses RAG to:  \n",
            "> 1. Ingest customer agreements (PDFs) ‚Üí  \n",
            "> 2. Retrieve clauses when a user asks \"Can I withdraw money?\" ‚Üí  \n",
            "> 3. Generate a precise answer: *\"Yes, but your account must be in good standing.\"* (from the retrieved document).\n",
            "\n",
            "---\n",
            "\n",
            "### ‚ö†Ô∏è Key Pitfalls to Avoid\n",
            "| Component          | Common Mistake                          | Fix                                  |\n",
            "|---------------------|------------------------------------------|---------------------------------------|\n",
            "| **Indexing**        | Using too large chunks ‚Üí slow retrieval  | Split docs into 500-1000 chars       |\n",
            "| **Retrieval**       | Returning irrelevant docs (low precision)| Tune `k` (top results) and embedding model |\n",
            "| **Generation**      | Over-reliance on retrieved docs         | Add fallback: \"I don't have this info\" |\n",
            "\n",
            "---\n",
            "\n",
            "### Summary Table\n",
            "| **Component**              | **Purpose**                                  | **Real-World Analogy**              |\n",
            "|----------------------------|----------------------------------------------|-------------------------------------|\n",
            "| Data Ingestion              | Collect raw data                             | Library shelves                    |\n",
            "| Preprocessing               | Clean & split documents                      | Sorting books into chapters        |\n",
            "| Indexing (Vector DB)        | Store vectors for fast search                | Index in a physical library        |\n",
            "| Retrieval System            | Find relevant docs for the query             | Librarian finding the right book   |\n",
            "| Generation Module (LLM)     | Create human-like answers with context        | Author writing a story using notes |\n",
            "| Evaluation Loop             | Measure accuracy & improve                   | Teacher grading & revising essays  |\n",
            "\n",
            "---\n",
            "\n",
            "### Final Thought\n",
            "**RAG isn't just \"an LLM + a database\"**‚Äîit's a *pipeline* where **retrieval** and **generation** work together to make answers **accurate, contextual, and trustworthy**. Start with **data ingestion ‚Üí preprocessing ‚Üí indexing** (the \"foundation\"), then build retrieval and generation on top. \n",
            "\n",
            "For beginners: **Use LangChain** (open-source framework) to implement RAG quickly. It handles most components out-of-the-box.\n",
            "\n",
            "Let me know if you'd like a **step-by-step tutorial** or **code example** for one of these components! üòä\n"
          ]
        }
      ],
      "source": [
        "import ollama\n",
        "\n",
        "response = ollama.chat(model='qwen3:4b', messages=[\n",
        "  {\n",
        "    'role': 'user',\n",
        "    'content': 'What are the main components of a RAG application?'\n",
        "  },\n",
        "])\n",
        "print(response['message']['content'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56Ocd-F9MsOp"
      },
      "source": [
        "Notice the output from OpenAI and Ollama is usually in Markdown format. You can request the output to be in plain text by setting the `response_format` parameter to `text`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBBQQGOhMsOp",
        "outputId": "5a41e9bb-987d-4061-cdc0-d3e6a681ee16"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Here are the **core components of a RAG (Retrieval-Augmented Generation) application**, explained clearly and concisely for both technical and non-technical audiences. I'll focus on *what* each component does and *why* it matters‚Äîavoiding excessive jargon while ensuring practical relevance.\n",
              "\n",
              "---\n",
              "\n",
              "### üß† 1. **Data Ingestion & Storage**\n",
              "- **What it does**: Collects, stores, and organizes raw data (e.g., PDFs, websites, databases, text files) into a structured format.\n",
              "- **Why it matters**: Without this, there's no data to retrieve. Real-world examples:  \n",
              "  ‚Üí *Customer support tickets* (for troubleshooting)  \n",
              "  ‚Üí *Internal knowledge bases* (for HR policies)  \n",
              "  ‚Üí *Research papers* (for academic answers)\n",
              "- **Key tools**: S3 buckets, databases (e.g., PostgreSQL), or cloud storage.\n",
              "\n",
              "---\n",
              "\n",
              "### üîç 2. **Document Preprocessing**\n",
              "- **What it does**: Cleans, splits, and transforms raw documents into a format suitable for retrieval (e.g., removing noise, splitting into chunks, standardizing text).\n",
              "- **Why it matters**: Prevents irrelevant results. Example:  \n",
              "  ‚Üí Splitting a 50-page PDF into 1000-character chunks so the model can find *exact* answers.  \n",
              "  ‚Üí Removing tables, images, or irrelevant sections from web pages.\n",
              "- **Key tools**: Python libraries like `PyPDF2`, `BeautifulSoup`, or `LangChain`'s chunkers.\n",
              "\n",
              "---\n",
              "\n",
              "### üì¶ 3. **Indexing (Vector Database)**\n",
              "- **What it does**: Converts processed documents into **vectors** (numerical representations) and stores them in a fast-searchable database.\n",
              "- **Why it matters**: This is the *heart* of RAG. Without it, you can't quickly find relevant documents.  \n",
              "  ‚Üí *Example*: When a user asks \"How do I reset my password?\", the system converts this query into a vector and searches the vector database for documents with similar vectors (e.g., \"password reset instructions\").\n",
              "- **Key tools**: FAISS, Pinecone, Weaviate, or ChromaDB.  \n",
              "  ‚Üí *Critical note*: This is where **embedding models** (e.g., `all-MiniLM-L6-v2`) live.\n",
              "\n",
              "---\n",
              "\n",
              "### üïµÔ∏è 4. **Retrieval System**\n",
              "- **What it does**: Takes a user query ‚Üí converts it into a vector ‚Üí searches the index ‚Üí returns *top-k* relevant documents.\n",
              "- **Why it matters**: This is where RAG *augments* the LLM. Without good retrieval, the LLM gets \"hallucinated\" answers (made up facts).  \n",
              "  ‚Üí *Example*: If the query is \"What is the capital of France?\", the system retrieves the document \"France: Paris is the capital\" (not a random document about \"France\" in a history book).\n",
              "- **Key metrics**: Precision (how many relevant docs?), Recall (how many relevant docs are found?).\n",
              "\n",
              "---\n",
              "\n",
              "### üß© 5. **Query Processing Pipeline**\n",
              "- **What it does**: Prepares the user query for retrieval (e.g., adding context, handling typos, language translation).\n",
              "- **Why it matters**: Ensures the query vector aligns with the index.  \n",
              "  ‚Üí *Example*: If a user types \"reset pass\", the system might correct it to \"reset password\" before retrieval to avoid mismatched results.\n",
              "\n",
              "---\n",
              "\n",
              "### ü§ñ 6. **Generation Module (LLM)**\n",
              "- **What it does**: Takes the retrieved documents + user query ‚Üí generates a **natural-language response**.\n",
              "- **Why it matters**: This is where the *augmentation* happens. The LLM uses retrieved info to answer *accurately* (e.g., \"Your password reset link is: `https://...`\").  \n",
              "  ‚Üí *Without RAG*: The LLM might hallucinate (\"Password reset links expire in 24 hours...\") because it has no recent data.  \n",
              "  ‚Üí *With RAG*: The LLM uses the retrieved document to say \"Your link expires in 1 hour\" (if the doc states this).\n",
              "- **Key tools**: LLMs like `GPT-4`, `Llama 3`, or `Claude`.\n",
              "\n",
              "---\n",
              "\n",
              "### üìä 7. **Evaluation & Feedback Loop**\n",
              "- **What it does**: Tests the RAG system's performance (e.g., accuracy, latency) and uses user feedback to improve.\n",
              "- **Why it matters**: RAG isn't a \"one-off\" solution. Real-world systems need constant tuning.  \n",
              "  ‚Üí *Example*: If users say \"This answer is wrong\", the system updates the index or retrieval model.\n",
              "- **Key metrics**:  \n",
              "  - **Precision**: % of retrieved docs that are relevant  \n",
              "  - **Relevance score**: How well the response matches the query  \n",
              "  - **Latency**: Time from query to response (critical for apps)\n",
              "\n",
              "---\n",
              "\n",
              "### üí° Why This Matters in Practice\n",
              "RAG solves a **real problem**: LLMs (like GPT) often hallucinate answers because they lack context. By *retrieving relevant documents first*, RAG:  \n",
              "‚úÖ **Reduces hallucinations** (uses actual data)  \n",
              "‚úÖ **Improves accuracy** (answers are grounded in your knowledge base)  \n",
              "‚úÖ **Scales better** (works with large datasets without retraining the LLM)\n",
              "\n",
              "> üåü **Real-world example**: A bank‚Äôs chatbot uses RAG to:  \n",
              "> 1. Ingest customer agreements (PDFs) ‚Üí  \n",
              "> 2. Retrieve clauses when a user asks \"Can I withdraw money?\" ‚Üí  \n",
              "> 3. Generate a precise answer: *\"Yes, but your account must be in good standing.\"* (from the retrieved document).\n",
              "\n",
              "---\n",
              "\n",
              "### ‚ö†Ô∏è Key Pitfalls to Avoid\n",
              "| Component          | Common Mistake                          | Fix                                  |\n",
              "|---------------------|------------------------------------------|---------------------------------------|\n",
              "| **Indexing**        | Using too large chunks ‚Üí slow retrieval  | Split docs into 500-1000 chars       |\n",
              "| **Retrieval**       | Returning irrelevant docs (low precision)| Tune `k` (top results) and embedding model |\n",
              "| **Generation**      | Over-reliance on retrieved docs         | Add fallback: \"I don't have this info\" |\n",
              "\n",
              "---\n",
              "\n",
              "### Summary Table\n",
              "| **Component**              | **Purpose**                                  | **Real-World Analogy**              |\n",
              "|----------------------------|----------------------------------------------|-------------------------------------|\n",
              "| Data Ingestion              | Collect raw data                             | Library shelves                    |\n",
              "| Preprocessing               | Clean & split documents                      | Sorting books into chapters        |\n",
              "| Indexing (Vector DB)        | Store vectors for fast search                | Index in a physical library        |\n",
              "| Retrieval System            | Find relevant docs for the query             | Librarian finding the right book   |\n",
              "| Generation Module (LLM)     | Create human-like answers with context        | Author writing a story using notes |\n",
              "| Evaluation Loop             | Measure accuracy & improve                   | Teacher grading & revising essays  |\n",
              "\n",
              "---\n",
              "\n",
              "### Final Thought\n",
              "**RAG isn't just \"an LLM + a database\"**‚Äîit's a *pipeline* where **retrieval** and **generation** work together to make answers **accurate, contextual, and trustworthy**. Start with **data ingestion ‚Üí preprocessing ‚Üí indexing** (the \"foundation\"), then build retrieval and generation on top. \n",
              "\n",
              "For beginners: **Use LangChain** (open-source framework) to implement RAG quickly. It handles most components out-of-the-box.\n",
              "\n",
              "Let me know if you'd like a **step-by-step tutorial** or **code example** for one of these components! üòä"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# you can use Markdown to make it look nicer in the notebook\n",
        "Markdown(response['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEPNrFTqMsOp"
      },
      "source": [
        "## 3. OpenAI Compatibility\n",
        "\n",
        "One of the coolest features of Ollama is that it is **OpenAI-compatible**.\n",
        "This means you can use the `openai` python client to talk to your local Ollama models! You just need to change the `base_url` to point to your local server.\n",
        "\n",
        "**Why is this useful?**\n",
        "It allows you to switch between expensive proprietary models (OpenAI) and free local models (Ollama) without rewriting your entire application logic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk1LqCs7MsOp",
        "outputId": "d0b480d0-6e2b-4c0c-85d2-d05c13293446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The **main components of a RAG (Retrieval-Augmented Generation) application** work together to enable AI models to generate **accurate, context-aware responses** by first retrieving relevant information from a *knowledge base* before generating text. Below is a clear breakdown of the core components, their roles, and how they interconnect:\n",
            "\n",
            "---\n",
            "\n",
            "### üîë 1. **Knowledge Base (KB)**\n",
            "   - **What it is**: A structured repository of source documents (e.g., PDFs, articles, databases, web pages).\n",
            "   - **Purpose**: Stores factual information that the RAG system uses for retrieval.\n",
            "   - **Examples**: \n",
            "     - Enterprise documents (internal reports, contracts)\n",
            "     - External knowledge (scientific papers, news articles)\n",
            "     - Structured datasets (databases, CSV files).\n",
            "\n",
            "---\n",
            "\n",
            "### üß† 2. **Embedding Model**\n",
            "   - **What it is**: A model that converts text (documents, queries) into **numerical vectors** (embeddings).\n",
            "   - **Purpose**: Enables semantic similarity comparisons between queries and documents.\n",
            "   - **Key Models**: \n",
            "     - `Sentence-BERT` (for text)\n",
            "     - `OpenAI`/`Claude` embeddings (for large-scale retrieval)\n",
            "     - Custom fine-tuned models (e.g., for domain-specific knowledge).\n",
            "\n",
            "---\n",
            "\n",
            "### üì¶ 3. **Vector Store**\n",
            "   - **What it is**: A database optimized for **efficient similarity search** (e.g., FAISS, Pinecone, Weaviate, Milvus).\n",
            "   - **Purpose**: Stores embeddings of documents and quickly retrieves the most relevant ones for a given query.\n",
            "   - **Why it matters**: Without this, retrieving relevant documents from large KBs would be computationally expensive.\n",
            "\n",
            "---\n",
            "\n",
            "### üîç 4. **Retrieval System**\n",
            "   - **What it is**: The component that **finds relevant documents** from the KB based on the user query.\n",
            "   - **How it works**:\n",
            "     1. User query ‚Üí Embedding model ‚Üí Vector\n",
            "     2. Vector store ‚Üí Returns top-N most similar documents (e.g., 3‚Äì5 relevant contexts)\n",
            "   - **Key Techniques**:\n",
            "     - **Dense retrieval**: Uses embedding vectors (e.g., via FAISS).\n",
            "     - **Hybrid retrieval**: Combines dense + keyword search (e.g., BM25).\n",
            "   - **Critical Metric**: *Relevance quality* (e.g., high precision = few wrong documents).\n",
            "\n",
            "---\n",
            "\n",
            "### üè≠ 5. **LLM (Language Model)**\n",
            "   - **What it is**: The core AI model that **generates responses** using the retrieved context.\n",
            "   - **Purpose**: Takes the retrieved documents + user query ‚Üí produces natural language answers.\n",
            "   - **Common Models**: \n",
            "     - `GPT-3.5`, `GPT-4`\n",
            "     - `Llama-2`, `Mistral` (for cost-effective deployment)\n",
            "     - **Specialized RAG models** (e.g., `Meta's Llama-3` with RAG fine-tuning).\n",
            "\n",
            "---\n",
            "\n",
            "### ‚öôÔ∏è 6. **Prompt Engineering & Context Formatting**\n",
            "   - **What it is**: How the retrieved documents are **structured** for the LLM.\n",
            "   - **Why it matters**: Poor formatting causes the LLM to ignore context or hallucinate.\n",
            "   - **Best Practices**:\n",
            "     - Use a **template** (e.g., `\"Here‚Äôs relevant info from our knowledge base: [doc1], [doc2]...`).\n",
            "     - Add **instructions** (e.g., \"Focus on the latest data; avoid speculation\").\n",
            "\n",
            "---\n",
            "\n",
            "### üîÑ 7. **RAG Pipeline (Workflow)**\n",
            "   - **The full process**:\n",
            "     ```\n",
            "     User Query ‚Üí [Embedding Model] ‚Üí Vector ‚Üí [Vector Store] ‚Üí Top-N Docs ‚Üí [Prompt Engineering] ‚Üí [LLM] ‚Üí Final Response\n",
            "     ```\n",
            "   - **Key Goal**: *Retrieve relevant context first ‚Üí Generate accurate response ‚Üí Avoid hallucinations*.\n",
            "\n",
            "---\n",
            "\n",
            "### üìä 8. **Evaluation Metrics** (Optional but critical for deployment)\n",
            "   - **Why include?**: Ensures RAG works reliably in practice.\n",
            "   - **Common Metrics**:\n",
            "     - **Relevance Score**: How well retrieved docs match the query.\n",
            "     - **Answer Accuracy**: % of correct answers from retrieved contexts.\n",
            "     - **Latency**: Time from query ‚Üí response (critical for real-time use).\n",
            "     - **Hallucination Rate**: % of answers that invent facts (reduces with better retrieval).\n",
            "\n",
            "---\n",
            "\n",
            "## üí° Why These Components Matter Together\n",
            "RAG **doesn‚Äôt replace LLMs**‚Äîit *enhances* them by injecting **up-to-date, domain-specific knowledge** from your KB. Without strong retrieval, the system hallucinates; without a good embedding model, it misses context. The magic happens when the **retrieval step is precise** (i.e., relevant documents are found quickly).\n",
            "\n",
            "### Real-World Example\n",
            "> **Scenario**: Building a healthcare chatbot that answers patient questions.\n",
            "> - **KB**: Patient records, clinical guidelines, FDA documents.\n",
            "> - **Retrieval**: Finds latest guidelines for a specific condition.\n",
            "> - **LLM**: Generates a safe, accurate response (e.g., \"For diabetic patients, monitor blood sugar every 2 hours...\").\n",
            "\n",
            "---\n",
            "\n",
            "### ‚ö†Ô∏è Key Takeaways for Implementation\n",
            "| Component          | Critical Focus                     |\n",
            "|---------------------|-------------------------------------|\n",
            "| Knowledge Base      | Quality > Quantity (only relevant docs) |\n",
            "| Embedding Model     | Domain-specific tuning for accuracy |\n",
            "| Vector Store        | Fast retrieval (low latency)       |\n",
            "| Retrieval System    | Balance relevance vs. recall       |\n",
            "| LLM                 | Fine-tune for RAG context handling  |\n",
            "\n",
            "üí° **Pro Tip**: Start simple‚Äîuse a **pre-trained embedding model** + **FAISS vector store** + **one LLM** (e.g., `GPT-3.5`). Most RAG projects fail due to poor retrieval, not model choice.\n",
            "\n",
            "This structure ensures your RAG app delivers **accurate, context-aware responses** without relying solely on generic LLM training (which can hallucinate). Let me know if you'd like a step-by-step implementation example! üöÄ\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Point the OpenAI client to the local Ollama server\n",
        "client = OpenAI(\n",
        "    base_url='http://localhost:11434/v1',\n",
        "    api_key='ollama', # Required, but ignored by Ollama\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"qwen3:4b\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What are the main components of a RAG application?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "response = response.choices[0].message.content\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HNcnyThMsOp",
        "outputId": "02b2f656-8e3f-4142-d0d2-fe98e3c3d02e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming markdown response:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Large language models (LLMs) are **a type of artificial intelligence (AI) system designed to understand, generate, and work with human language**. They‚Äôre trained on massive amounts of text data (like books, articles, code, and web content) to predict patterns in language and produce coherent, contextually relevant responses. Here‚Äôs a breakdown in simple terms:\n",
              "\n",
              "---\n",
              "\n",
              "### üîë **Core Idea: What They Do**\n",
              "Think of LLMs as **extremely powerful \"language pattern detectives.\"** They:\n",
              "1. **Learn language patterns**: By analyzing vast text data, they understand how words relate to each other (e.g., \"cat\" ‚Üí \"meow,\" \"run\" ‚Üí \"fast\").\n",
              "2. **Predict the next word**: When given a partial sentence (e.g., \"The sky is...\"), they guess the most likely next word(s).\n",
              "3. **Generate new text**: Using this pattern-matching, they create new sentences, stories, code, emails, or answers to questions ‚Äî all while maintaining natural flow.\n",
              "\n",
              "> ‚úÖ **Real-world example**:  \n",
              "> *Input*: \"Explain quantum computing in simple terms.\"  \n",
              "> *LLM response*: \"Quantum computing uses the principles of quantum mechanics... [and so on]\"\n",
              "\n",
              "---\n",
              "\n",
              "### üìè **Why \"Large\"? The Key Size Matters**\n",
              "- **Scale = Power**: LLMs are called \"large\" because they have **billions to trillions of parameters** (values the model adjusts during training).  \n",
              "  - *Why this matters*: More parameters = better pattern recognition = more accurate language generation.  \n",
              "  - *Example*: GPT-3.5 has ~175 billion parameters; ChatGPT-4 has ~1 trillion+.\n",
              "- **Training data**: They‚Äôre trained on **huge datasets** (e.g., trillions of words from the internet, books, code repositories).  \n",
              "  - *Important*: The model **doesn‚Äôt \"understand\" language** ‚Äî it statistically predicts patterns based on what it learned.\n",
              "\n",
              "---\n",
              "\n",
              "### üß† **How They Work (Simplified)**\n",
              "1. **Input**: A user asks a question or provides text (e.g., \"How do I bake a cake?\").  \n",
              "2. **Processing**: The model scans the input, identifies context, and predicts the most probable next words.  \n",
              "3. **Output**: Generates a response (e.g., step-by-step cake instructions).  \n",
              "*Under the hood*: They use a **neural network architecture** (specifically, **transformers** ‚Äî which excel at handling sequence data like text).\n",
              "\n",
              "---\n",
              "\n",
              "### üí° **What LLMs Can Do (Practical Uses)**\n",
              "| **Task**                     | **Example**                                      |\n",
              "|------------------------------|--------------------------------------------------|\n",
              "| Answer questions              | \"What's the capital of France?\" ‚Üí \"Paris\"         |\n",
              "| Write stories, emails, scripts | Draft a sci-fi story or professional email       |\n",
              "| Code generation               | Write Python code from a description             |\n",
              "| Translate languages           | Translate Spanish to English                     |\n",
              "| Summarize text                | Turn a 10-page report into a 1-page summary      |\n",
              "| Debugging/fixing errors       | Help spot mistakes in code or logic              |\n",
              "\n",
              "---\n",
              "\n",
              "### ‚ö†Ô∏è **Key Limitations (What They *Can't* Do)**\n",
              "- **Don‚Äôt \"understand\" meaning**: They mimic language patterns but don‚Äôt grasp concepts (e.g., they won‚Äôt explain *why* gravity works).  \n",
              "- **Not creative**: They generate based on patterns, not true innovation (e.g., they won‚Äôt invent new ideas).  \n",
              "- **Bias risks**: If training data has biases (e.g., gender, race), LLMs can amplify them.  \n",
              "- **Hallucinations**: They may invent facts not in their training data (e.g., \"In 2023, the moon landed on Earth\").  \n",
              "- **Security**: They can be tricked with prompts (\"What's the best way to hack?\") ‚Üí **Never use LLMs for malicious purposes**.\n",
              "\n",
              "---\n",
              "\n",
              "### üåü **Why Are LLMs So Important?**\n",
              "- **Democratize AI**: Tools like ChatGPT make powerful AI accessible to non-experts.  \n",
              "- **Transform industries**: Used for coding (GitHub Copilot), healthcare (medical reports), education, and more.  \n",
              "- **Pushing AI boundaries**: LLMs are the most advanced \"language-focused\" AI systems built so far ‚Äî but they‚Äôre still evolving.\n",
              "\n",
              "---\n",
              "\n",
              "### üíé **In a Nutshell**\n",
              "> **Large language models (LLMs) are hyper-advanced AI systems trained on massive text data to predict and generate human-like language. They‚Äôre not \"conscious\" but excel at tasks like answering questions, writing, coding, and translating ‚Äî by spotting patterns in language, not understanding meaning. Their size (billions of parameters) enables high performance but also comes with limitations like bias and hallucinations.**\n",
              "\n",
              "They‚Äôre a powerful tool for real-world applications today ‚Äî **but they‚Äôre not sentient, and they shouldn‚Äôt replace human judgment or creativity**.\n",
              "\n",
              "If you'd like to dive deeper into *how* they work, *specific examples* (like ChatGPT vs. Gemini), or *how to use them safely*, just say the word! üòä"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# you can also use streaming as well:\n",
        "\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "client = OpenAI(\n",
        "    base_url='http://localhost:11434/v1',\n",
        "    api_key='ollama', # Required, but ignored by Ollama\n",
        ")\n",
        "stream = client.chat.completions.create(\n",
        "        model=\"qwen3:4b\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \"Explain what are large language models.\"}\n",
        "        ],\n",
        "        stream=True,  # Enable streaming\n",
        "    )\n",
        "\n",
        "print(\"Streaming markdown response:\")\n",
        "full_response_content = \"\"\n",
        "display_handle = display(Markdown(\"\"), display_id=True)\n",
        "\n",
        "for chunk in stream:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content:\n",
        "        full_response_content += content\n",
        "        display_handle.update(Markdown(full_response_content))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf--vBvjMsOp"
      },
      "source": [
        "## 3. Activity: Build an AI Email Assistant\n",
        "\n",
        "**Objective**: Expand the functionality of a basic AI email generator.\n",
        "\n",
        "**The Scenario**: You have a simple script that drafts professional emails. Your task is to customize it to handle more specific details like tone and dates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FvUVWbGMsOp",
        "outputId": "a8788537-94de-4ef8-f1d6-169aa723dc16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Email Assistant ---\n",
            "\n",
            "Generating email...\n",
            "\n",
            "Subject: I need a vacation  \n",
            "\n",
            "Dear Sara Smith,  \n",
            "\n",
            "I hope this message finds you well.  \n",
            "\n",
            "I am writing to formally request a short vacation period as I have been feeling increasingly tired and need time to unwind. After several months of consistent work demands, I believe a brief break will help me recharge and return to our projects with greater focus and energy.  \n",
            "\n",
            "I am flexible with the timing and would appreciate your guidance on the best window to accommodate this request without disrupting team workflows. Please let me know what dates would work for you, and I‚Äôm happy to align my schedule accordingly.  \n",
            "\n",
            "Thank you for your understanding and support‚ÄîI truly appreciate your assistance in helping me find a solution that benefits both my well-being and our team‚Äôs success.  \n",
            "\n",
            "Best regards,  \n",
            "[Your Name]\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# 1. Setup the Client\n",
        "# Ensure you have the model pulled: !ollama pull qwen3:4b\n",
        "client = OpenAI(\n",
        "    base_url='http://localhost:11434/v1',\n",
        "    api_key='ollama',\n",
        ")\n",
        "\n",
        "def generate_email(subject, recipient_name, additional_info):\n",
        "    \"\"\"\n",
        "    Generates an email based on the provided inputs.\n",
        "    \"\"\"\n",
        "    # 1. Construct the prompt\n",
        "    prompt = f\"Write a professional email to {recipient_name} with the subject '{subject}'. Include the following information: {additional_info}\"\n",
        "\n",
        "    # 2. Call the API\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"qwen3:4b\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# 3. Test the Function with User Input\n",
        "print(\"--- Email Assistant ---\")\n",
        "entry_recipient = input(\"Enter recipient name: \")\n",
        "entry_subject = input(\"Enter email subject: \")\n",
        "entry_info = input(\"Enter key points/details: \")\n",
        "\n",
        "print(\"\\nGenerating email...\\n\")\n",
        "print(generate_email(entry_subject, entry_recipient, entry_info))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tXsD-1LMsOp"
      },
      "source": [
        "### üü¢ Challenges\n",
        "\n",
        "Now that you have a functioning template, modify the code above to solve the following challenges:\n",
        "\n",
        "**1. Add a `tone` argument**\n",
        "Customize the style of the email.\n",
        "*   Modify the testing block to ask the user for a `tone` (e.g., \"urgent\", \"enthusiastic\").\n",
        "*   Pass this argument to `generate_email` and update the prompt.\n",
        "\n",
        "**2. Add `start_date` and `duration`**\n",
        "Imagine this is a \"Request for Leave\" email generator.\n",
        "*   Add inputs for `start_date` and `duration`.\n",
        "*   Update the prompt to ensure these details are included clearly in the email.\n",
        "\n",
        "**3. Dynamic System Prompt**\n",
        "Currently, the system prompt is static (\"You are a helpful assistant.\").\n",
        "Change the system prompt to: *\"You are a professional executive assistant who negotiates schedules effectively.\"* or allow it to be passed as an argument.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}