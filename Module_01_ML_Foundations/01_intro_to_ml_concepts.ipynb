{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z01A93lCFmXt"
      },
      "source": [
        "# Module 1: Foundations of AI and Machine Learning\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this module, you will be able to:\n",
        "- Understand the difference between rule-based programming and machine learning\n",
        "- Distinguish between supervised and unsupervised learning\n",
        "- Understand the basic ML development workflow\n",
        "- Grasp key concepts: data preprocessing, features, overfitting, and evaluation metrics\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMARHUYSFmXt"
      },
      "source": [
        "## 1. Rule-Based Programming vs Machine Learning\n",
        "\n",
        "### Traditional Programming (Rule-Based)\n",
        "In traditional programming, we explicitly write rules to solve problems:\n",
        "\n",
        "```\n",
        "Input + Rules â†’ Output\n",
        "```\n",
        "\n",
        "**Example:** A spam filter with explicit rules:\n",
        "- If email contains \"lottery winner\" â†’ spam\n",
        "- If email contains \"Nigerian prince\" â†’ spam\n",
        "- If sender is in contacts â†’ not spam\n",
        "\n",
        "### Machine Learning Approach\n",
        "Instead of writing rules, we let the computer **learn patterns** from data:\n",
        "\n",
        "```\n",
        "Input + Expected Output â†’ Learned Rules (Model)\n",
        "```\n",
        "\n",
        "**Example:** A spam filter that learns from thousands of labeled emails what patterns indicate spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTuvJqlSFmXt"
      },
      "outputs": [],
      "source": [
        "# Rule-Based Example: Temperature Classification\n",
        "def classify_temperature_rules(temp_celsius):\n",
        "    \"\"\"Traditional rule-based approach\"\"\"\n",
        "    if temp_celsius < 0:\n",
        "        return \"freezing\"\n",
        "    elif temp_celsius < 15:\n",
        "        return \"cold\"\n",
        "    elif temp_celsius < 25:\n",
        "        return \"comfortable\"\n",
        "    elif temp_celsius < 35:\n",
        "        return \"warm\"\n",
        "    else:\n",
        "        return \"hot\"\n",
        "\n",
        "# Test the rule-based system\n",
        "test_temps = [-5, 10, 22, 30, 40]\n",
        "for temp in test_temps:\n",
        "    print(f\"{temp}Â°C â†’ {classify_temperature_rules(temp)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsNyo8XoFmXu"
      },
      "source": [
        "### When to Use ML vs Rule-Based?\n",
        "\n",
        "| Use Rule-Based When | Use ML When |\n",
        "|---------------------|-------------|\n",
        "| Rules are clear and well-defined | Patterns are complex or unknown |\n",
        "| Domain is stable (rules don't change) | Data evolves over time |\n",
        "| Limited edge cases | Too many rules to enumerate |\n",
        "| Explainability is critical | Performance matters more than interpretability |\n",
        "\n",
        "**Generative AI Context:** Large Language Models (LLMs) are the ultimate ML approach - they learn language patterns from billions of text examples rather than explicitly coded grammar rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5uKTycVFmXu"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Supervised vs Unsupervised Learning\n",
        "\n",
        "### Supervised Learning\n",
        "The model learns from **labeled examples** - we provide both the input AND the correct answer.\n",
        "\n",
        "| Input (Features) | Output (Label) |\n",
        "|------------------|----------------|\n",
        "| Email text | Spam/Not Spam |\n",
        "| House size, location | Price |\n",
        "| Image of animal | Cat/Dog |\n",
        "\n",
        "**Types:**\n",
        "- **Classification:** Predict a category (spam/not spam, cat/dog)\n",
        "- **Regression:** Predict a continuous value (price, temperature)\n",
        "\n",
        "### Unsupervised Learning\n",
        "The model finds **patterns without labels** - we only provide input data.\n",
        "\n",
        "**Types:**\n",
        "- **Clustering:** Group similar items (customer segments)\n",
        "- **Dimensionality Reduction:** Compress data while preserving structure\n",
        "- **Anomaly Detection:** Find unusual patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x5WWaR5FmXu"
      },
      "outputs": [],
      "source": [
        "# Install required packages (run once)\n",
        "!pip install -q scikit-learn matplotlib numpy pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EZGokL2FmXu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs, load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Generate sample data for clustering\n",
        "X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=0.8, random_state=42)\n",
        "\n",
        "# Unsupervised Learning: K-Means Clustering (no labels provided!)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "y_pred = kmeans.fit_predict(X)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "axes[0].scatter(X[:, 0], X[:, 1], c='gray', alpha=0.6)\n",
        "axes[0].set_title('Raw Data (No Labels)', fontsize=12)\n",
        "axes[0].set_xlabel('Feature 1')\n",
        "axes[0].set_ylabel('Feature 2')\n",
        "\n",
        "axes[1].scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', alpha=0.6)\n",
        "axes[1].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "                c='red', marker='X', s=200, label='Centroids')\n",
        "axes[1].set_title('After K-Means Clustering (Patterns Discovered)', fontsize=12)\n",
        "axes[1].set_xlabel('Feature 1')\n",
        "axes[1].set_ylabel('Feature 2')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ’¡ Key insight: The algorithm found 3 groups WITHOUT being told what the groups are!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWqOu-6NFmXu"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. The Machine Learning Workflow\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   Collect   â”‚ â†’ â”‚   Clean &   â”‚ â†’ â”‚   Feature   â”‚ â†’ â”‚   Train     â”‚ â†’ â”‚  Evaluate   â”‚\n",
        "â”‚    Data     â”‚   â”‚  Preprocess â”‚   â”‚ Engineering â”‚   â”‚   Model     â”‚   â”‚   Model     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### Step 1: Data Collection\n",
        "- Gather relevant data for your problem\n",
        "- More quality data â†’ better models\n",
        "\n",
        "### Step 2: Data Preprocessing\n",
        "- Handle missing values\n",
        "- Remove duplicates\n",
        "- Normalize/standardize numerical features\n",
        "- Encode categorical variables\n",
        "\n",
        "### Step 3: Feature Engineering\n",
        "- Select relevant features\n",
        "- Create new features from existing ones\n",
        "- Transform features for better model performance\n",
        "\n",
        "### Step 4: Model Training\n",
        "- Split data into training and test sets\n",
        "- Choose an algorithm\n",
        "- Train the model on training data\n",
        "\n",
        "### Step 5: Model Evaluation\n",
        "- Test on held-out data\n",
        "- Calculate performance metrics\n",
        "- Iterate and improve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mcZLi4UFmXu"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the famous Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "print(\"Dataset Overview:\")\n",
        "print(f\"   Features: {iris.feature_names}\")\n",
        "print(f\"   Classes: {iris.target_names}\")\n",
        "print(f\"   Samples: {X.shape[0]}, Features per sample: {X.shape[1]}\")\n",
        "\n",
        "# Step 1: Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "print(f\"\\nData Split: {len(X_train)} training, {len(X_test)} test samples\")\n",
        "\n",
        "# Step 2: Preprocess - Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"Features standardized (mean=0, std=1)\")\n",
        "\n",
        "# Step 3: Train the model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "print(\"Model trained!\")\n",
        "\n",
        "# Step 4: Evaluate\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nTest Accuracy: {accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF77lAM7FmXu"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Key Concepts\n",
        "\n",
        "### Overfitting: The \"Memorization\" Problem\n",
        "\n",
        "**Overfitting** occurs when a model learns the training data TOO well, including noise and outliers, and fails to generalize to new data.\n",
        "\n",
        "```\n",
        "                 Underfitting          Good Fit           Overfitting\n",
        "                 (Too simple)          (Just right)       (Too complex)\n",
        "                     â†“                     â†“                   â†“\n",
        "Training Accuracy:  Low                   High                 Very High\n",
        "Test Accuracy:      Low                   High                 Low\n",
        "```\n",
        "\n",
        "**How to Prevent Overfitting:**\n",
        "- Use more training data\n",
        "- Use simpler models\n",
        "- Apply regularization\n",
        "- Use cross-validation\n",
        "\n",
        "### Common Evaluation Metrics\n",
        "\n",
        "For **Classification:**\n",
        "- **Accuracy:** % of correct predictions\n",
        "- **Precision:** Of predicted positives, how many are actually positive?\n",
        "- **Recall:** Of actual positives, how many did we find?\n",
        "- **F1-Score:** Harmonic mean of precision and recall\n",
        "\n",
        "For **Regression:**\n",
        "- **MAE (Mean Absolute Error):** Average absolute difference\n",
        "- **MSE (Mean Squared Error):** Average squared difference\n",
        "- **RÂ² Score:** How much variance is explained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWDh6QI2FmXu"
      },
      "outputs": [],
      "source": [
        "# Demonstrating Overfitting with Polynomial Regression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate simple data with noise\n",
        "np.random.seed(42)\n",
        "X_simple = np.sort(np.random.rand(20, 1) * 10, axis=0)\n",
        "y_simple = np.sin(X_simple).ravel() + np.random.randn(20) * 0.3\n",
        "\n",
        "# Test on new points\n",
        "X_test_simple = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "degrees = [1, 4, 15]  # Low, Good, Overfit\n",
        "titles = ['Underfitting (Degree 1)', 'Good Fit (Degree 4)', 'Overfitting (Degree 15)']\n",
        "\n",
        "for ax, degree, title in zip(axes, degrees, titles):\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly.fit_transform(X_simple)\n",
        "    X_test_poly = poly.transform(X_test_simple)\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly, y_simple)\n",
        "    y_pred_simple = model.predict(X_test_poly)\n",
        "\n",
        "    ax.scatter(X_simple, y_simple, color='blue', label='Training data')\n",
        "    ax.plot(X_test_simple, y_pred_simple, color='red', label=f'Model (degree {degree})')\n",
        "    ax.set_title(title, fontsize=11)\n",
        "    ax.set_ylim(-2, 2)\n",
        "    ax.legend(fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ðŸ’¡ Notice: The rightmost model perfectly fits training points but misses the true pattern!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FZgtQSHFmXu"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Why This Matters for Generative AI\n",
        "\n",
        "Understanding these ML fundamentals is crucial because:\n",
        "\n",
        "1. **LLMs are ML Models:** They learn patterns from massive text datasets (supervised on next-word prediction)\n",
        "\n",
        "2. **Overfitting exists in LLMs:** Called \"memorization\" - models may reproduce training data verbatim\n",
        "\n",
        "3. **Evaluation is Key:** Just like we evaluate classifiers with metrics, we evaluate LLMs with metrics like perplexity, BLEU, etc.\n",
        "\n",
        "4. **Fine-tuning is Transfer Learning:** When we fine-tune an LLM on specific data, we're applying transfer learning principles\n",
        "\n",
        "5. **Feature Engineering â†’ Prompt Engineering:** In GenAI, how you structure your input (prompt) is like feature engineering for traditional ML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NgaYAFQFmXu"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ“ Student Exercise\n",
        "\n",
        "### Challenge: Build Your Own Classifier\n",
        "\n",
        "Using what you've learned, complete the following tasks:\n",
        "\n",
        "1. Load the breast cancer dataset from scikit-learn\n",
        "2. Split it into training and test sets\n",
        "3. Train a classifier of your choice\n",
        "4. Evaluate using accuracy and print a classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etfsyb5uFmXu"
      },
      "outputs": [],
      "source": [
        "# Student Exercise: Complete the code below\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "print(f\"Dataset: {data.filename if hasattr(data, 'filename') else 'Breast Cancer'}\")\n",
        "print(f\"Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
        "print(f\"Classes: {data.target_names}\")\n",
        "\n",
        "# TODO: Split the data (80% train, 20% test)\n",
        "# X_train, X_test, y_train, y_test = ...\n",
        "\n",
        "# TODO: Scale the features using StandardScaler\n",
        "# scaler = ...\n",
        "\n",
        "# TODO: Train a classifier (try LogisticRegression or RandomForestClassifier)\n",
        "# model = ...\n",
        "\n",
        "# TODO: Make predictions and calculate accuracy\n",
        "# accuracy = ...\n",
        "# print(f\"Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "# TODO: Print classification report\n",
        "# from sklearn.metrics import classification_report\n",
        "# print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDwu6jQ9FmXu"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸŽ¯ Key Takeaways\n",
        "\n",
        "1. **Machine Learning** finds patterns in data rather than following explicit rules\n",
        "2. **Supervised learning** needs labeled data; **unsupervised learning** discovers structure without labels\n",
        "3. The ML workflow: Collect â†’ Preprocess â†’ Engineer Features â†’ Train â†’ Evaluate\n",
        "4. **Overfitting** = memorizing training data instead of learning generalizable patterns\n",
        "5. Always **evaluate on held-out test data** to measure true performance\n",
        "\n",
        "---\n",
        "\n",
        "### Next Module: Deep Learning Primer â†’\n",
        "We'll explore neural networks - the foundation of modern generative AI!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}